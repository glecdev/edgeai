# GLEC DTG Edge AI SDK - Recursive Improvement Workflow
## ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ Claude Code ê°œë°œ ì›Œí¬í”Œë¡œìš°

---

## ğŸ¯ í•µì‹¬ ì² í•™

### 1. ì¬ê·€ì  ê°œì„  (Recursive Improvement)
```
Plan â†’ Implement â†’ Test â†’ Review â†’ Improve â†’ Document â†’ Commit
  â†“                                                          â†“
  â†â†â†â†â†â†â†â†â†â†â†â† Learn & Iterate â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†â†
```

**ì›ì¹™**:
- ëª¨ë“  ì‘ì—…ì€ **Plan â†’ Execute â†’ Validate â†’ Improve** ì‚¬ì´í´ë¡œ ì§„í–‰
- ê° ì‚¬ì´í´ì—ì„œ ì–»ì€ **í•™ìŠµì„ ë‹¤ìŒ ì‚¬ì´í´ì— ì ìš©**
- **ì‹¤íŒ¨ëŠ” í•™ìŠµì˜ ê¸°íšŒ** - ì‹¤íŒ¨ íŒ¨í„´ì„ ë¬¸ì„œí™”í•˜ê³  íšŒí”¼ ì „ëµ ìˆ˜ë¦½

### 2. ê¸°ìˆ  ì§€ì‹ ë…¸í•˜ìš° ì¦ê°• (Knowledge Augmentation)
```
Code â†’ Memory MCP â†’ Pattern Library â†’ Best Practices â†’ Reuse
```

**ì „ëµ**:
- **Memory MCP**: ì„¤ê³„ ê²°ì •, ì‹¤í—˜ ê²°ê³¼, ìµœì  ì„¤ì • ì €ì¥
- **Pattern Library**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì½”ë“œ íŒ¨í„´ ì¶”ì¶œ
- **Best Practices**: í”„ë¡œì íŠ¸ íŠ¹í™” ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì •ë¦½
- **Continuous Learning**: ê° ì‘ì—…ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ì²´ê³„í™”

### 3. ì»¨í…ìŠ¤íŠ¸ ì„ ëª…í•¨ ìœ ì§€ (Context Clarity)
```
Clear Structure + Consistent Naming + State Management + Documentation
```

**ë°©ë²•**:
- **ëª…í™•í•œ í´ë” êµ¬ì¡°**: ì—­í• ë³„ ë””ë ‰í† ë¦¬ ë¶„ë¦¬
- **ì¼ê´€ëœ ë„¤ì´ë°**: ì»¨ë²¤ì…˜ ì¤€ìˆ˜ (snake_case, camelCase, PascalCase)
- **ìƒíƒœ ê´€ë¦¬**: Todo, Git, Memory MCPë¡œ ì§„í–‰ ìƒí™© ì¶”ì 
- **ë¬¸ì„œ-ì½”ë“œ ë™ê¸°í™”**: ì½”ë“œ ë³€ê²½ ì‹œ ë¬¸ì„œ ìë™ ì—…ë°ì´íŠ¸

---

## ğŸ”„ 7-Phase Recursive Workflow

### Phase 1ï¸âƒ£: PLAN (ê³„íš)

**ëª©í‘œ**: ì‘ì—…ì„ ëª…í™•íˆ ì´í•´í•˜ê³  ìµœì ì˜ ì ‘ê·¼ ë°©ì‹ ì„¤ê³„

**í™œë™**:
1. **Task ë¶„ì„**
   ```bash
   # What: ë¬´ì—‡ì„ ë§Œë“¤ ê²ƒì¸ê°€?
   # Why: ì™œ í•„ìš”í•œê°€?
   # How: ì–´ë–»ê²Œ êµ¬í˜„í•  ê²ƒì¸ê°€?
   # Dependencies: ì„ í–‰ ì‘ì—…ì€?
   # Risks: ì˜ˆìƒ ìœ„í—˜ì€?
   ```

2. **ì•„í‚¤í…ì²˜ ì„¤ê³„**
   - ì»´í¬ë„ŒíŠ¸ ë‹¤ì´ì–´ê·¸ë¨ ì‘ì„±
   - ë°ì´í„° í”Œë¡œìš° ì •ì˜
   - API ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„

3. **Memory MCPì— ì €ì¥**
   ```json
   {
     "entity": "design_decision",
     "name": "TCN_architecture",
     "observations": [
       "1D Conv with dilation for temporal modeling",
       "Residual connections for gradient flow",
       "3 layers, 64 filters each"
     ]
   }
   ```

4. **Todo ìƒì„±**
   - êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì‘ì—…ìœ¼ë¡œ ë¶„í•´
   - ê° ì‘ì—…ì— ì˜ˆìƒ ì‹œê°„ ë° ìš°ì„ ìˆœìœ„ ë¶€ì—¬

**ì¶œë ¥ë¬¼**:
- Architecture diagram (draw.io, PlantUML)
- API specification (OpenAPI)
- Todo list (TodoWrite)
- Design decisions (Memory MCP)

**Quality Gate**:
- [ ] ëª¨ë“  ìš”êµ¬ì‚¬í•­ì´ ì„¤ê³„ì— ë°˜ì˜ë˜ì—ˆëŠ”ê°€?
- [ ] ì•„í‚¤í…ì²˜ê°€ í™•ì¥ ê°€ëŠ¥í•œê°€?
- [ ] ì˜ì¡´ì„±ì´ ëª…í™•í•œê°€?

---

### Phase 2ï¸âƒ£: IMPLEMENT (êµ¬í˜„)

**ëª©í‘œ**: ê³ í’ˆì§ˆ ì½”ë“œ ì‘ì„± with TDD

**í™œë™**:
1. **Test-First Development**
   ```python
   # 1. í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„± (Red)
   def test_tcn_output_shape():
       model = TCN(input_dim=10, output_dim=1)
       x = torch.randn(32, 60, 10)  # batch, seq, features
       y = model(x)
       assert y.shape == (32, 1)

   # 2. ìµœì†Œ êµ¬í˜„ (Green)
   class TCN(nn.Module):
       # ... implementation

   # 3. ë¦¬íŒ©í† ë§ (Refactor)
   ```

2. **Skill í™œìš©**
   ```bash
   # ë°˜ë³µ ì‘ì—… ìë™í™”
   ./.claude/skills/train-model/run.sh tcn --epochs 100
   ```

3. **ì‹¤ì‹œê°„ ê²€ì¦**
   ```bash
   # Watch modeë¡œ í…ŒìŠ¤íŠ¸ ìë™ ì‹¤í–‰
   pytest-watch tests/
   ```

4. **ì ì§„ì  ì»¤ë°‹**
   ```bash
   # ì‘ì€ ë‹¨ìœ„ë¡œ ìì£¼ ì»¤ë°‹
   git add -p  # Interactive staging
   git commit -m "feat(tcn): Add dilated convolution layer"
   ```

**ì¶œë ¥ë¬¼**:
- Production code
- Unit tests (>80% coverage)
- Integration tests
- Git commits (semantic)

**Quality Gate**:
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•˜ëŠ”ê°€?
- [ ] ì½”ë“œ ì»¤ë²„ë¦¬ì§€ >80%ì¸ê°€?
- [ ] Linter ì—ëŸ¬ê°€ ì—†ëŠ”ê°€?
- [ ] ì½”ë“œê°€ ê°€ë…ì„±ì´ ìˆëŠ”ê°€?

---

### Phase 3ï¸âƒ£: TEST (í…ŒìŠ¤íŠ¸)

**ëª©í‘œ**: ë‹¤ì¸µì  í…ŒìŠ¤íŠ¸ë¡œ í’ˆì§ˆ ë³´ì¥

**í™œë™**:
1. **Unit Test** (ê°œë³„ í•¨ìˆ˜/í´ë˜ìŠ¤)
   ```python
   # ai-models/tests/test_tcn.py
   def test_tcn_forward_pass():
       """TCN forward pass produces correct shape"""
       model = TCN(input_dim=10, output_dim=1, num_layers=3)
       x = torch.randn(32, 60, 10)
       y = model(x)
       assert y.shape == (32, 1)
       assert not torch.isnan(y).any()
   ```

2. **Integration Test** (ì»´í¬ë„ŒíŠ¸ ê°„ í†µì‹ )
   ```python
   def test_can_to_uart_communication():
       """End-to-end: CAN â†’ STM32 â†’ UART â†’ Android"""
       # STM32 ì‹œë®¬ë ˆì´í„°
       stm32_sim = STM32Simulator()

       # CAN ë©”ì‹œì§€ ì „ì†¡
       can_msg = CANMessage(id=0x123, data=[0x10, 0x20])
       stm32_sim.send_can(can_msg)

       # UART ìˆ˜ì‹  í™•ì¸
       uart_data = stm32_sim.read_uart()
       assert uart_data.startswith(b'\xAA')  # START marker
   ```

3. **Performance Benchmark**
   ```python
   def test_tcn_inference_latency():
       """TCN inference completes within 25ms"""
       model = TCN(...)
       x = torch.randn(1, 60, 10)

       import time
       start = time.time()
       with torch.no_grad():
           y = model(x)
       latency = (time.time() - start) * 1000

       assert latency < 25  # Target: <25ms
   ```

4. **Coverage Analysis**
   ```bash
   pytest --cov=ai_models --cov-report=html
   open htmlcov/index.html
   ```

**ì¶œë ¥ë¬¼**:
- Test results
- Coverage report (>80%)
- Performance benchmarks
- Test documentation

**Quality Gate**:
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼?
- [ ] Coverage >80%?
- [ ] ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±? (<25ms, <2W, >85%)
- [ ] Edge case ì²˜ë¦¬?

---

### Phase 4ï¸âƒ£: REVIEW (ê²€í† )

**ëª©í‘œ**: ì½”ë“œ í’ˆì§ˆ, ì•„í‚¤í…ì²˜, ë³´ì•ˆ ê²€ì¦

**í™œë™**:
1. **ìë™ ì½”ë“œ ë¦¬ë·°** (Skill ì‚¬ìš©)
   ```bash
   ./.claude/skills/code-review/run.sh --target ai-models/training/
   ```

2. **ì •ì  ë¶„ì„**
   ```bash
   # Python
   pylint ai_models/
   mypy ai_models/
   bandit -r ai_models/  # Security

   # Android
   ./gradlew lint
   ./gradlew detekt
   ```

3. **ì•„í‚¤í…ì²˜ ì¼ê´€ì„± í™•ì¸**
   - ì„¤ê³„ ë¬¸ì„œì™€ ì½”ë“œ ì¼ì¹˜ ì—¬ë¶€
   - SOLID ì›ì¹™ ì¤€ìˆ˜
   - DRY (Don't Repeat Yourself) ìœ„ë°˜ ê²€ì‚¬

4. **ë³´ì•ˆ ê²€í† **
   ```bash
   # Dependency vulnerability scan
   pip-audit

   # Android
   ./gradlew dependencyCheckAnalyze
   ```

5. **ë¬¸ì„œ ë™ê¸°í™”**
   - ì½”ë“œ ì£¼ì„ ì™„ì„±ë„
   - API ë¬¸ì„œ ìµœì‹ ì„±
   - README ì •í™•ì„±

**ì¶œë ¥ë¬¼**:
- Code review report
- Security audit report
- Architecture compliance report
- Documentation gaps list

**Quality Gate**:
- [ ] Linter ì—ëŸ¬ 0ê°œ?
- [ ] ë³´ì•ˆ ì·¨ì•½ì  ì—†ìŒ?
- [ ] ì•„í‚¤í…ì²˜ ì¼ê´€ì„± ìœ ì§€?
- [ ] ë¬¸ì„œê°€ ì½”ë“œì™€ ë™ê¸°í™”?

---

### Phase 5ï¸âƒ£: IMPROVE (ê°œì„ )

**ëª©í‘œ**: ì½”ë“œ í’ˆì§ˆ í–¥ìƒ ë° ìµœì í™”

**í™œë™**:
1. **ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§**
   ```bash
   # AI ëª¨ë¸
   python -m cProfile -o profile.stats train_tcn.py
   snakeviz profile.stats

   # Android
   # Snapdragon Profiler ì‚¬ìš©
   ```

2. **ì½”ë“œ ë¦¬íŒ©í† ë§**
   ```python
   # Before (ë³µì¡ë„ ë†’ìŒ)
   def process_data(data):
       if data is not None:
           if len(data) > 0:
               if data[0] > 0:
                   return data[0] * 2
       return None

   # After (ë³µì¡ë„ ë‚®ìŒ)
   def process_data(data):
       if not data or data[0] <= 0:
           return None
       return data[0] * 2
   ```

3. **ê¸°ìˆ  ë¶€ì±„ í•´ê²°**
   - TODO ì£¼ì„ ì²˜ë¦¬
   - FIXME í•´ê²°
   - HACK ì œê±° ë° ì •ìƒí™”

4. **ì¬ì‚¬ìš© íŒ¨í„´ ì¶”ì¶œ**
   ```python
   # ê³µí†µ íŒ¨í„´ì„ ìœ í‹¸ë¦¬í‹°ë¡œ ì¶”ì¶œ
   # utils/model_utils.py
   def load_and_quantize_model(path, quantization='int8'):
       """Reusable pattern for model loading"""
       model = torch.load(path)
       if quantization == 'int8':
           model = quantize_int8(model)
       return model
   ```

5. **Memory MCPì— í•™ìŠµ ì €ì¥**
   ```json
   {
     "entity": "optimization_result",
     "name": "tcn_quantization",
     "observations": [
       "INT8 quantization: 4x size reduction",
       "Accuracy loss: only 1.2%",
       "Inference speed: 3x faster",
       "Best config: PTQ with 500 calibration samples"
     ]
   }
   ```

**ì¶œë ¥ë¬¼**:
- Refactored code
- Performance report (before/after)
- Pattern library updates
- Memory MCP entries

**Quality Gate**:
- [ ] ì„±ëŠ¥ ê°œì„  ì¸¡ì • ê°€ëŠ¥?
- [ ] ë³µì¡ë„ ê°ì†Œ?
- [ ] ì¬ì‚¬ìš©ì„± ì¦ê°€?
- [ ] ê¸°ìˆ  ë¶€ì±„ ê°ì†Œ?

---

### Phase 6ï¸âƒ£: DOCUMENT (ë¬¸ì„œí™”)

**ëª©í‘œ**: ì§€ì‹ ì²´ê³„í™” ë° ê³µìœ 

**í™œë™**:
1. **ì½”ë“œ ì£¼ì„**
   ```python
   def train_tcn(config: TrainConfig) -> ModelMetrics:
       """Train Temporal Convolutional Network for fuel prediction.

       Args:
           config: Training configuration containing:
               - epochs: Number of training epochs (default: 100)
               - batch_size: Batch size (default: 64)
               - learning_rate: Learning rate (default: 0.001)

       Returns:
           ModelMetrics containing:
               - train_accuracy: Training set accuracy
               - val_accuracy: Validation set accuracy
               - model_size_mb: Model size in MB
               - inference_time_ms: Average inference time

       Raises:
           ValueError: If config is invalid
           RuntimeError: If training fails

       Example:
           >>> config = TrainConfig(epochs=100)
           >>> metrics = train_tcn(config)
           >>> print(f"Accuracy: {metrics.val_accuracy}%")
       """
   ```

2. **API ë¬¸ì„œ ìƒì„±**
   ```bash
   # Python
   pdoc --html ai_models -o docs/api

   # Android
   ./gradlew dokkaHtml
   ```

3. **ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨**
   ```plantuml
   @startuml
   component "CAN Bus" as CAN
   component "STM32" as STM32
   component "Android App" as Android
   component "AI Engine" as AI

   CAN --> STM32 : CAN messages (1Hz)
   STM32 --> Android : UART (921600 baud)
   Android --> AI : Inference (60s window)
   AI --> Android : Predictions
   @enduml
   ```

4. **Changelog ì—…ë°ì´íŠ¸**
   ```markdown
   ## [1.2.0] - 2025-01-09

   ### Added
   - TCN model with INT8 quantization
   - LSTM-Autoencoder for anomaly detection
   - LightGBM for behavior classification

   ### Changed
   - Improved inference speed by 3x
   - Reduced model size by 75%

   ### Fixed
   - Memory leak in JNI bridge
   - CAN message parsing edge cases
   ```

5. **CLAUDE.md ì—…ë°ì´íŠ¸**
   ```bash
   ./.claude/skills/update-docs/run.sh --target CLAUDE.md
   ```

**ì¶œë ¥ë¬¼**:
- Code comments (docstrings)
- API documentation
- Architecture diagrams
- Changelog
- Updated CLAUDE.md

**Quality Gate**:
- [ ] ëª¨ë“  public API ë¬¸ì„œí™”?
- [ ] ë‹¤ì´ì–´ê·¸ë¨ì´ ìµœì‹  ì•„í‚¤í…ì²˜ ë°˜ì˜?
- [ ] Changelog ì—…ë°ì´íŠ¸?
- [ ] CLAUDE.md ë™ê¸°í™”?

---

### Phase 7ï¸âƒ£: COMMIT (ì»¤ë°‹)

**ëª©í‘œ**: ë²„ì „ ê´€ë¦¬ ë° ë°°í¬ ì¤€ë¹„

**í™œë™**:
1. **Semantic Commit**
   ```bash
   # Conventional Commits ì‚¬ìš©
   git commit -m "feat(tcn): Add INT8 quantization support

   - Implement post-training quantization (PTQ)
   - Add calibration dataset support (500 samples)
   - Achieve 4x size reduction with 1.2% accuracy loss

   Performance:
   - Model size: 12MB â†’ 3MB
   - Inference: 60ms â†’ 20ms
   - Accuracy: 89.7% â†’ 88.5%

   BREAKING CHANGE: Requires SNPE SDK 2.35.0+

   Closes #42"
   ```

2. **Git Tag (ë²„ì „)**
   ```bash
   git tag -a v1.2.0 -m "Release v1.2.0: TCN quantization"
   git push origin v1.2.0
   ```

3. **Changelog ìƒì„±**
   ```bash
   # Conventional Commits â†’ Changelog
   npx conventional-changelog -p angular -i CHANGELOG.md -s
   ```

4. **CI/CD íŠ¸ë¦¬ê±°**
   ```bash
   git push origin main
   # â†’ GitHub Actions ìë™ ì‹¤í–‰
   # â†’ Tests, Build, Deploy
   ```

**ì¶œë ¥ë¬¼**:
- Git commits (semantic)
- Git tags (version)
- Updated CHANGELOG.md
- CI/CD pipeline execution

**Quality Gate**:
- [ ] Commit messageê°€ ì»¨ë²¤ì…˜ ì¤€ìˆ˜?
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼?
- [ ] CI/CD íŒŒì´í”„ë¼ì¸ ì„±ê³µ?
- [ ] ë²„ì „ íƒœê·¸ ìƒì„±?

---

## ğŸ” ì¬ê·€ì  í•™ìŠµ ë£¨í”„ (Recursive Learning Loop)

### ì‚¬ì´í´ 1: ê¸°ë³¸ êµ¬í˜„
```
Plan â†’ Implement â†’ Test â†’ Review â†’ Improve â†’ Document â†’ Commit
Output: Working prototype (70% quality)
Learning: Basic architecture, pain points identified
```

### ì‚¬ì´í´ 2: ê°œì„  êµ¬í˜„
```
Plan (refined) â†’ Implement (optimized) â†’ Test (comprehensive) â†’ ...
Output: Production-ready (85% quality)
Learning: Performance bottlenecks, optimization techniques
```

### ì‚¬ì´í´ 3: ìµœì í™” êµ¬í˜„
```
Plan (data-driven) â†’ Implement (best practices) â†’ ...
Output: Optimized solution (95% quality)
Learning: Edge cases, best configurations, reusable patterns
```

### í•™ìŠµ ì €ì¥ (Memory MCP)
```json
{
  "cycle": 3,
  "improvements": [
    "Batch size 64 â†’ 128 improved throughput by 40%",
    "Learning rate 0.001 â†’ 0.0005 stabilized training",
    "Data augmentation increased accuracy by 3%"
  ],
  "patterns": [
    "Always use learning rate scheduling",
    "Monitor validation loss for early stopping",
    "Use mixed precision for 2x speedup"
  ]
}
```

---

## ğŸ“Š í’ˆì§ˆ ë©”íŠ¸ë¦­ (Quality Metrics)

### ì½”ë“œ í’ˆì§ˆ
- **Coverage**: >80% (ëª©í‘œ: 90%)
- **Complexity**: Cyclomatic complexity <10
- **Duplication**: <3%
- **Maintainability Index**: >20

### ì„±ëŠ¥
- **AI Inference**: <50ms (ëª©í‘œ: <30ms)
- **Power**: <2W
- **Model Size**: <100MB (ëª©í‘œ: <20MB)
- **Accuracy**: >85% (ëª©í‘œ: >90%)

### í”„ë¡œì„¸ìŠ¤
- **Cycle Time**: Plan â†’ Commit <1 day (ì‘ì€ ì‘ì—…)
- **Lead Time**: ìš”ì²­ â†’ ë°°í¬ <1 week
- **Deployment Frequency**: ì£¼ 1íšŒ ì´ìƒ
- **Change Failure Rate**: <5%

---

## ğŸ›  ë„êµ¬ í†µí•© (Tool Integration)

### Memory MCP
```bash
# ì„¤ê³„ ê²°ì • ì €ì¥
curl -X POST http://localhost:3000/entities \
  -d '{"name": "tcn_architecture", "entityType": "design_decision", "observations": ["..."]}'

# ì‹¤í—˜ ê²°ê³¼ ì¡°íšŒ
curl http://localhost:3000/entities?entityType=experiment_result
```

### MLflow
```python
with mlflow.start_run(run_name="tcn_v1.2.0"):
    mlflow.log_param("quantization", "int8")
    mlflow.log_metric("accuracy", 88.5)
    mlflow.pytorch.log_model(model, "model")
```

### Git
```bash
# Semantic versioning
git tag -a v1.2.0 -m "TCN quantization release"

# Conventional commits
git commit -m "feat(tcn): Add quantization support"
```

### DVC
```bash
# Data versioning
dvc add data/training_set.csv
dvc push

# Reproduce experiment
dvc repro
```

---

## ğŸ¯ ì„±ê³µ ì‚¬ë¡€ í…œí”Œë¦¿

### Task: TCN ëª¨ë¸ INT8 ì–‘ìí™”

**Phase 1: Plan**
- Target: ëª¨ë¸ í¬ê¸° 75% ê°ì†Œ, ì •í™•ë„ ì†ì‹¤ <2%
- Approach: Post-Training Quantization (PTQ)
- Risks: ì •í™•ë„ ì €í•˜, ì§€ì›ë˜ì§€ ì•ŠëŠ” ì—°ì‚°ì

**Phase 2: Implement**
```python
# tests/test_quantization.py
def test_quantized_model_accuracy():
    original = load_model("tcn_fp32.pth")
    quantized = quantize_int8(original)

    acc_original = evaluate(original, test_set)
    acc_quantized = evaluate(quantized, test_set)

    accuracy_loss = acc_original - acc_quantized
    assert accuracy_loss < 2.0  # Target: <2%
```

**Phase 3: Test**
- Unit: âœ… Quantization reduces size by 75%
- Integration: âœ… SNPE DLC conversion successful
- Performance: âœ… Inference 20ms (target <25ms)

**Phase 4: Review**
- Code quality: âœ… Pylint score 9.5/10
- Security: âœ… No vulnerabilities
- Architecture: âœ… Consistent with design

**Phase 5: Improve**
- Optimization: Calibration samples 500 â†’ 1000 (accuracy +0.5%)
- Refactor: Extract `quantize_int8()` to utils

**Phase 6: Document**
```python
def quantize_int8(model: nn.Module, calibration_data: Dataset) -> nn.Module:
    """Apply INT8 post-training quantization.

    Reduces model size by ~75% with <2% accuracy loss.

    Args:
        model: PyTorch model to quantize
        calibration_data: Representative dataset (500-1000 samples)

    Returns:
        Quantized model ready for SNPE conversion
    """
```

**Phase 7: Commit**
```bash
git commit -m "feat(tcn): Add INT8 quantization support

- Implement PTQ with 1000 calibration samples
- Achieve 75% size reduction (12MB â†’ 3MB)
- Accuracy loss: only 1.2% (89.7% â†’ 88.5%)
- Inference speed: 3x faster (60ms â†’ 20ms)

Closes #42"
```

**Learning** (Memory MCP):
```json
{
  "entity": "best_practice",
  "name": "quantization_workflow",
  "observations": [
    "1000 calibration samples optimal (vs 500)",
    "Always test on real device (SNPE)",
    "Monitor outlier activations during calibration",
    "PTQ sufficient for TCN (QAT not needed)"
  ]
}
```

---

## ğŸš€ Quick Start

### 1. ìƒˆ ì‘ì—… ì‹œì‘
```bash
# 1. Todo ìƒì„±
echo "Implement TCN quantization" | claude-code

# 2. Memory MCPì—ì„œ ê´€ë ¨ ì§€ì‹ ì¡°íšŒ
curl http://localhost:3000/entities?entityType=best_practice&name=quantization

# 3. Plan ì‘ì„±
vi docs/plans/tcn_quantization.md

# 4. Implement with TDD
pytest-watch tests/test_quantization.py
```

### 2. ì‚¬ì´í´ ì‹¤í–‰
```bash
# Implement â†’ Test â†’ Review
./.claude/skills/train-model/run.sh tcn --quantize int8
./.claude/skills/run-tests/run.sh ai
./.claude/skills/code-review/run.sh --target ai-models/

# Improve â†’ Document â†’ Commit
./.claude/skills/optimize-performance/run.sh --model tcn
./.claude/skills/update-docs/run.sh
git add -A && git commit
```

### 3. í•™ìŠµ ì €ì¥
```bash
# Memory MCPì— ê²°ê³¼ ì €ì¥
echo '{
  "entity": "experiment_result",
  "name": "tcn_quantization_v1",
  "observations": ["..."]
}' | http POST localhost:3000/entities
```

---

## ğŸ“ˆ ì§€ì†ì  ê°œì„  (Continuous Improvement)

### ì£¼ê°„ íšŒê³  (Weekly Retrospective)
```markdown
## Week 1 Retrospective

### What went well?
- TCN quantization achieved better than expected results
- Test coverage increased to 85%
- No production incidents

### What could be improved?
- Documentation lagged behind code changes
- Some edge cases not covered in tests
- Build time increased to 15 minutes

### Action items:
- [ ] Set up automatic documentation generation
- [ ] Add property-based testing for edge cases
- [ ] Optimize Docker build caching
```

### ì›”ê°„ ë©”íŠ¸ë¦­ ë¦¬ë·° (Monthly Metrics Review)
```
Code Quality Trend:
  Coverage: 75% â†’ 80% â†’ 85% â†—ï¸
  Complexity: 8.5 â†’ 7.2 â†’ 6.8 â†—ï¸
  Tech Debt: 45 â†’ 38 â†’ 32 hours â†—ï¸

Performance Trend:
  Inference: 60ms â†’ 30ms â†’ 20ms â†—ï¸
  Model Size: 48MB â†’ 12MB â†’ 3MB â†—ï¸
  Accuracy: 85.3% â†’ 88.5% â†’ 89.7% â†—ï¸
```

### Pattern Library ì„±ì¥
```
Iteration 1: 5 patterns documented
Iteration 2: 12 patterns documented (+140%)
Iteration 3: 18 patterns documented (+50%)
â†’ Knowledge compounding!
```

---

## ğŸ“ í•µì‹¬ ì›ì¹™ (Core Principles)

1. **Small, Frequent Iterations**
   - í° ì‘ì—…ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í•´
   - ë§¤ì¼ ê°€ì‹œì  ì§„ì „
   - ë¹ ë¥¸ í”¼ë“œë°± ë£¨í”„

2. **Test Everything**
   - í…ŒìŠ¤íŠ¸ ì—†ëŠ” ì½”ë“œ = ê¸°ìˆ  ë¶€ì±„
   - Coverage >80% í•„ìˆ˜
   - Performance í…ŒìŠ¤íŠ¸ í¬í•¨

3. **Document as You Go**
   - ì½”ë“œ ì‘ì„± ì¤‘ ì¦‰ì‹œ ë¬¸ì„œí™”
   - ë‚˜ì¤‘ìœ¼ë¡œ ë¯¸ë£¨ì§€ ì•Šê¸°
   - ë¬¸ì„œ = ë¯¸ë˜ì˜ ë‚˜ë¥¼ ìœ„í•œ íˆ¬ì

4. **Learn from Failures**
   - ì‹¤íŒ¨ë¥¼ Memory MCPì— ê¸°ë¡
   - íšŒí”¼ ì „ëµ ìˆ˜ë¦½
   - íŒ€ê³¼ ê³µìœ 

5. **Automate Repetition**
   - ê°™ì€ ì‘ì—… 2ë²ˆ í•˜ë©´ Skill ë§Œë“¤ê¸°
   - CI/CDë¡œ ìˆ˜ë™ ì‘ì—… ì œê±°
   - ì‚¬ëŒì€ ì°½ì˜ì  ì‘ì—…ì— ì§‘ì¤‘

---

ì´ ì›Œí¬í”Œë¡œìš°ë¥¼ ë”°ë¥´ë©´:
- âœ… **60-70% ë¹ ë¥¸ ê°œë°œ ì†ë„**
- âœ… **95%+ ì½”ë“œ í’ˆì§ˆ**
- âœ… **ì§€ì† ê°€ëŠ¥í•œ ê¸°ìˆ  ì„±ì¥**
- âœ… **ëª…í™•í•œ ì»¨í…ìŠ¤íŠ¸ ìœ ì§€**
- âœ… **ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ ì¶•ì **

ğŸ¯ **ëª©í‘œ: ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ Edge AI SDK ê°œë°œ!**
