# ğŸš€ ë¡œì»¬ GPU ì‹¤í–‰ ê°€ì´ë“œ - PyTorch + Miniconda

**GLEC DTG EdgeAI - ì‹¤ì œ GPU í•™ìŠµ ì‹¤í–‰**

**ëª©ì **: ë¡œì»¬ GPUì—ì„œ PyTorchì™€ Minicondaë¥¼ í™œìš©í•œ ì‹¤ì œ ëª¨ë¸ í•™ìŠµ
**ëŒ€ìƒ**: ë¡œì»¬ ê°œë°œì (NVIDIA GPU ë³´ìœ )
**ì˜ˆìƒ ì‹œê°„**: ì´ 6-10ì‹œê°„ (í™˜ê²½ êµ¬ì¶• 1ì‹œê°„ + í•™ìŠµ 5-9ì‹œê°„)

---

## ğŸ“‹ ì‚¬ì „ ìš”êµ¬ì‚¬í•­ ì²´í¬

### í•„ìˆ˜ í•˜ë“œì›¨ì–´
- âœ… **NVIDIA GPU**: RTX 3060 ì´ìƒ (VRAM 12GB ê¶Œì¥)
- âœ… **RAM**: 16GB ì´ìƒ (32GB ê¶Œì¥)
- âœ… **ì €ì¥ê³µê°„**: 100GB ì´ìƒ ì—¬ìœ  (SSD ê¶Œì¥)
- âœ… **ì „ì›**: ë…¸íŠ¸ë¶ì˜ ê²½ìš° AC ì–´ëŒ‘í„° ì—°ê²° í•„ìˆ˜

### í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
- âœ… **Windows 11** ë˜ëŠ” **Ubuntu 22.04+**
- âœ… **NVIDIA Driver**: ìµœì‹  ë²„ì „ (535.x ì´ìƒ)
- âœ… **Miniconda**: Python ê°€ìƒí™˜ê²½ ê´€ë¦¬
- âœ… **Git**: ì½”ë“œ ë²„ì „ ê´€ë¦¬

---

## 1ï¸âƒ£ í™˜ê²½ êµ¬ì¶• (30ë¶„-1ì‹œê°„)

### Step 1.1: NVIDIA Driver í™•ì¸

```bash
# NVIDIA GPU í™•ì¸
nvidia-smi

# ì˜ˆìƒ ì¶œë ¥:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 535.154.05   Driver Version: 535.154.05   CUDA Version: 12.2   |
# |-------------------------------+----------------------+----------------------+
# | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
# |   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  N/A |
# +-----------------------------------------------------------------------------+
```

**ë§Œì•½ nvidia-smiê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë©´**:
```bash
# Windows: NVIDIA ê³µì‹ ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  ë“œë¼ì´ë²„ ë‹¤ìš´ë¡œë“œ
# https://www.nvidia.com/Download/index.aspx

# Ubuntu:
sudo apt update
sudo apt install nvidia-driver-535
sudo reboot
```

---

### Step 1.2: Miniconda ì„¤ì¹˜

**Windows**:
```powershell
# Miniconda ë‹¤ìš´ë¡œë“œ (PowerShell)
Invoke-WebRequest -Uri "https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe" -OutFile "miniconda.exe"

# ì„¤ì¹˜ ì‹¤í–‰ (GUI ë”°ë¼ì„œ ì§„í–‰)
.\miniconda.exe

# ì„¤ì¹˜ í›„ PowerShell ì¬ì‹œì‘
```

**Ubuntu/Linux**:
```bash
# Miniconda ë‹¤ìš´ë¡œë“œ
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

# ì„¤ì¹˜
bash Miniconda3-latest-Linux-x86_64.sh

# ì‰˜ ì„¤ì • ì ìš©
source ~/.bashrc

# í™•ì¸
conda --version
# ì¶œë ¥: conda 24.1.2 (ë˜ëŠ” ìµœì‹  ë²„ì „)
```

---

### Step 1.3: Conda í™˜ê²½ ìƒì„±

```bash
# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd d:\edgeai\edgeai-repo
# ë˜ëŠ” Linux: cd ~/edgeai/edgeai-repo

# Conda í™˜ê²½ ìƒì„± (Python 3.10)
conda create -n dtg-ai python=3.10 -y

# í™˜ê²½ í™œì„±í™”
conda activate dtg-ai

# í™•ì¸
python --version
# ì¶œë ¥: Python 3.10.13
```

**ì¤‘ìš”**: ì´í›„ ëª¨ë“  ëª…ë ¹ì–´ëŠ” `dtg-ai` í™˜ê²½ì´ í™œì„±í™”ëœ ìƒíƒœì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”!

---

### Step 1.4: PyTorch ì„¤ì¹˜ (CUDA 11.8)

```bash
# dtg-ai í™˜ê²½ì´ í™œì„±í™”ëœ ìƒíƒœì—ì„œ:
conda activate dtg-ai

# PyTorch 2.2.0 + CUDA 11.8 ì„¤ì¹˜
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118

# ì„¤ì¹˜ í™•ì¸ (ì•½ 2-3ë¶„ ì†Œìš”)
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"

# ì˜ˆìƒ ì¶œë ¥:
# PyTorch: 2.2.0+cu118
# CUDA available: True
# GPU: NVIDIA GeForce RTX 3060
```

**ë§Œì•½ CUDA available: Falseê°€ ë‚˜ì˜¨ë‹¤ë©´**:
1. NVIDIA ë“œë¼ì´ë²„ ì¬ì„¤ì¹˜
2. PyTorch ì¬ì„¤ì¹˜: `pip uninstall torch torchvision torchaudio` í›„ ë‹¤ì‹œ ì„¤ì¹˜
3. ì‹œìŠ¤í…œ ì¬ë¶€íŒ…

---

### Step 1.5: í”„ë¡œì íŠ¸ ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# edgeai-repo ë””ë ‰í† ë¦¬ì—ì„œ:
cd d:\edgeai\edgeai-repo

# ëª¨ë“  ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì£¼ìš” íŒ¨í‚¤ì§€ í™•ì¸
pip list | grep -E "numpy|pandas|lightgbm|onnx|mlflow|pyyaml|scikit-learn"

# ì˜ˆìƒ ì¶œë ¥:
# lightgbm         4.3.0
# mlflow           2.15.1
# numpy            1.26.4
# onnx             1.16.0
# pandas           2.2.0
# PyYAML           6.0.1
# scikit-learn     1.4.0
```

---

### Step 1.6: í™˜ê²½ ê²€ì¦

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ suite ì‹¤í–‰ (í™˜ê²½ ê²€ì¦)
cd d:\edgeai\edgeai-repo

# Windows:
python -m pytest tests/ -v --tb=no -q --ignore=tests\e2e_test.py --ignore=tests\benchmark_inference.py --ignore=tests\data_validator.py

# Linux:
python -m pytest tests/ -v --tb=no -q --ignore=tests/e2e_test.py --ignore=tests/benchmark_inference.py --ignore=tests/data_validator.py

# ì˜ˆìƒ ì¶œë ¥:
# ============================== 159 passed in 15-20s ==============================
```

**âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•˜ë©´ í™˜ê²½ êµ¬ì¶• ì™„ë£Œ!**

---

## 2ï¸âƒ£ ë°ì´í„° ìƒì„± (10-30ë¶„)

### Step 2.1: í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±

```bash
cd d:\edgeai\edgeai-repo\data-generation

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (ë¹ ë¥¸ ê²€ì¦ìš©, 1,000 ìƒ˜í”Œ, ì•½ 2-3ë¶„)
python -c "
import sys
sys.path.append('..')
from ai-models.utils.synthetic_simulator import generate_dataset
import numpy as np

print('ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...')

# Train: 800 ìƒ˜í”Œ, 0% anomaly (LSTM-AEëŠ” ì •ìƒ ë°ì´í„°ë§Œ í•™ìŠµ!)
X_train, y_fuel_train, y_anomaly_train = generate_dataset(
    num_samples=800,
    duration_minutes=1.0,
    patterns=['highway_cruise', 'city_traffic'],
    anomaly_ratio=0.0,  # ì¤‘ìš”: 0% anomaly!
    sampling_rate_hz=1.0
)

# Val: 150 ìƒ˜í”Œ, 10% anomaly (threshold calibrationìš©)
X_val, y_fuel_val, y_anomaly_val = generate_dataset(
    num_samples=150,
    duration_minutes=1.0,
    patterns=['highway_cruise', 'city_traffic'],
    anomaly_ratio=0.1,
    sampling_rate_hz=1.0
)

# Test: 150 ìƒ˜í”Œ, 10% anomaly
X_test, y_fuel_test, y_anomaly_test = generate_dataset(
    num_samples=150,
    duration_minutes=1.0,
    patterns=['highway_cruise', 'city_traffic'],
    anomaly_ratio=0.1,
    sampling_rate_hz=1.0
)

# CSV ì €ì¥
import pandas as pd
import os

os.makedirs('../datasets', exist_ok=True)

feature_names = [
    'vehicle_speed', 'engine_rpm', 'throttle_position',
    'brake_pressure', 'coolant_temp', 'fuel_level',
    'acceleration_x', 'acceleration_y', 'acceleration_z',
    'steering_angle', 'gps_lat'
]

def save_dataset(X, y_fuel, y_anomaly, filename):
    # Reshape X: (num_samples, sequence_length, features) -> (total_timesteps, features)
    num_samples, seq_len, num_features = X.shape
    X_flat = X.reshape(-1, num_features)

    # Create DataFrame
    df = pd.DataFrame(X_flat, columns=feature_names)

    # Add timestamp
    df['timestamp'] = np.tile(np.arange(seq_len), num_samples)

    # Add targets (repeat for each timestep in sequence)
    df['fuel_consumption'] = np.repeat(y_fuel, seq_len)
    df['carbon_emission'] = df['fuel_consumption'] * 2.31  # CO2 conversion
    df['label'] = np.repeat(['anomaly' if a == 1 else 'normal' for a in y_anomaly], seq_len)

    # Save
    df.to_csv(filename, index=False)
    print(f'âœ… Saved: {filename} ({len(df)} rows)')

save_dataset(X_train, y_fuel_train, y_anomaly_train, '../datasets/train.csv')
save_dataset(X_val, y_fuel_val, y_anomaly_val, '../datasets/val.csv')
save_dataset(X_test, y_fuel_test, y_anomaly_test, '../datasets/test.csv')

print('âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!')
print(f'   Train: {len(y_anomaly_train)} samples, {y_anomaly_train.mean()*100:.1f}% anomaly')
print(f'   Val:   {len(y_anomaly_val)} samples, {y_anomaly_val.mean()*100:.1f}% anomaly')
print(f'   Test:  {len(y_anomaly_test)} samples, {y_anomaly_test.mean()*100:.1f}% anomaly')
"
```

**ì˜ˆìƒ ì¶œë ¥**:
```
ğŸš€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...
INFO:__main__:Generated 100/800 samples
INFO:__main__:Generated 200/800 samples
...
âœ… Saved: ../datasets/train.csv (48000 rows)
âœ… Saved: ../datasets/val.csv (9000 rows)
âœ… Saved: ../datasets/test.csv (9000 rows)
âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!
   Train: 800 samples, 0.0% anomaly
   Val:   150 samples, 10.0% anomaly
   Test:  150 samples, 10.0% anomaly
```

---

### Step 2.2: Production ë°ì´í„°ì…‹ ìƒì„± (ì„ íƒ, ì•½ 20-30ë¶„)

**í…ŒìŠ¤íŠ¸ í•™ìŠµì´ ì„±ê³µí•˜ë©´** production ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±:

```python
# Production: 10,000 ìƒ˜í”Œ (ì‹¤ì œ í•™ìŠµìš©)
# ì‹¤í–‰ ì‹œê°„: ì•½ 20-30ë¶„

python -c "
import sys
sys.path.append('..')
from ai-models.utils.synthetic_simulator import generate_dataset
import numpy as np
import pandas as pd

print('ğŸš€ Production ë°ì´í„°ì…‹ ìƒì„± ì¤‘ (20-30ë¶„ ì†Œìš”)...')

# Train: 8,000 ìƒ˜í”Œ, 0% anomaly
X_train, y_fuel_train, y_anomaly_train = generate_dataset(
    num_samples=8000,
    duration_minutes=1.0,
    patterns=['highway_cruise', 'city_traffic'],
    anomaly_ratio=0.0,
    sampling_rate_hz=1.0
)

# Val: 1,500 ìƒ˜í”Œ, 10% anomaly
X_val, y_fuel_val, y_anomaly_val = generate_dataset(
    num_samples=1500,
    duration_minutes=1.0,
    patterns=['highway_cruise', 'city_traffic'],
    anomaly_ratio=0.1,
    sampling_rate_hz=1.0
)

# Test: 1,500 ìƒ˜í”Œ, 10% anomaly
X_test, y_fuel_test, y_anomaly_test = generate_dataset(
    num_samples=1500,
    duration_minutes=1.0,
    patterns=['highway_cruise', 'city_traffic'],
    anomaly_ratio=0.1,
    sampling_rate_hz=1.0
)

# (save_dataset í•¨ìˆ˜ëŠ” ìœ„ì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©)
# ...
"
```

---

## 3ï¸âƒ£ GPU ëª¨ë¸ í•™ìŠµ (5-9ì‹œê°„)

### Step 3.1: TCN ëª¨ë¸ í•™ìŠµ (2-4ì‹œê°„)

```bash
cd d:\edgeai\edgeai-repo\ai-models\training

# TCN í•™ìŠµ ì‹œì‘ (batch_size=64, epochs=100)
python train_tcn.py --config ../config.yaml --epochs 100 --batch-size 64

# ì˜ˆìƒ ì¶œë ¥:
# Training on device: cuda
# [INFO] Training without MLflow logging (MLflow ì—†ì–´ë„ ì§„í–‰ë¨)
# Loading datasets...
# Creating model...
# Model parameters: 412928
# Starting training...
# Epoch 1/100 | Train Loss: 0.3245 | Val Loss: 0.2891 | RÂ² Score: 0.6234 | Time: 45.23s
# Epoch 2/100 | Train Loss: 0.2456 | Val Loss: 0.2345 | RÂ² Score: 0.7123 | Time: 43.12s
# ...
# Early stopping at epoch 42
# Training completed! Best validation loss: 0.1234
# Model saved: models/tcn_fuel_best.pth
```

**í•™ìŠµ ëª¨ë‹ˆí„°ë§**:
```bash
# ë‹¤ë¥¸ í„°ë¯¸ë„ì—ì„œ GPU ì‚¬ìš©ë¥  ì‹¤ì‹œê°„ í™•ì¸
watch -n 1 nvidia-smi

# ì˜ˆìƒ ì¶œë ¥:
# | GPU  Name        | GPU-Util | Memory-Usage |
# | NVIDIA RTX 3060  |  95%     | 8192 / 12288 MiB |
```

**í•™ìŠµ ì¤‘ ë¬¸ì œ ë°œìƒ ì‹œ**:

**OOM (Out of Memory) ì—ëŸ¬**:
```bash
# Batch size ì¤„ì´ê¸°
python train_tcn.py --config ../config.yaml --epochs 100 --batch-size 32
```

**ëŠë¦° í•™ìŠµ ì†ë„** (epochë‹¹ >5ë¶„):
```bash
# num_workers ì¡°ì • (DataLoader)
# train_tcn.py íŒŒì¼ ìˆ˜ì •: num_workers=0 â†’ num_workers=4
```

---

### Step 3.2: LSTM-AE ëª¨ë¸ í•™ìŠµ (2-4ì‹œê°„)

```bash
cd d:\edgeai\edgeai-repo\ai-models\training

# LSTM-AE í•™ìŠµ ì‹œì‘
python train_lstm_ae.py --config ../config.yaml --epochs 100 --batch-size 64

# ì˜ˆìƒ ì¶œë ¥:
# Training on device: cuda
# Loading datasets...
# Creating model...
# Model parameters: 156672
# Calculating anomaly threshold...
# Anomaly threshold: 0.0234
# Starting training...
# Epoch 1/100 | Train Loss: 0.0456 | Val Loss: 0.0389 | F1: 0.6234 | Precision: 0.5890 | Recall: 0.6612 | Time: 52.34s
# Epoch 2/100 | Train Loss: 0.0312 | Val Loss: 0.0289 | F1: 0.7456 | Precision: 0.7234 | Recall: 0.7689 | Time: 51.23s
# ...
# Early stopping at epoch 38
# Training completed! Best validation loss: 0.0156
# Model saved: models/lstm_ae_best.pth
```

**ì¤‘ìš”**: LSTM-AEëŠ” ì •ìƒ ë°ì´í„°ë§Œ í•™ìŠµ (train.csv anomaly_ratio=0%)
- Validationì—ì„œ anomaly ë°ì´í„°ë¡œ threshold ë³´ì •
- F1-Score > 0.85 ëª©í‘œ

---

### Step 3.3: LightGBM ì¬í•™ìŠµ (ì„ íƒ, 30ì´ˆ)

```bash
cd d:\edgeai\edgeai-repo\ai-models\training

# LightGBM í•™ìŠµ (CPUë§Œìœ¼ë¡œ ì¶©ë¶„, ë§¤ìš° ë¹ ë¦„)
python train_lightgbm.py --config ../config.yaml

# ì˜ˆìƒ ì¶œë ¥:
# Training LightGBM model...
# [LightGBM] [Info] Total Bins 2550
# [LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 11
# Training until validation scores don't improve for 50 rounds
# [100]	valid_0's multi_logloss: 0.0234	valid_0's multi_error: 0.0045
# ...
# Early stopping, best iteration is: [234]
# Training complete! Accuracy: 99.54%
# Model saved: models/lightgbm_behavior_model.txt
```

**ì´ë¯¸ 99.54% ì •í™•ë„ ë‹¬ì„±**: ì¬í•™ìŠµì€ ì„ íƒì‚¬í•­

---

## 4ï¸âƒ£ ëª¨ë¸ ìµœì í™” (1-2ì‹œê°„)

### Step 4.1: PyTorch â†’ ONNX ë³€í™˜

**TCN ëª¨ë¸ ë³€í™˜**:
```python
cd d:\edgeai\edgeai-repo\ai-models\conversion

python -c "
import torch
import sys
sys.path.append('..')
from training.train_tcn import TCN

# ëª¨ë¸ ë¡œë“œ
device = torch.device('cuda')
model = TCN(input_dim=11, output_dim=1, num_channels=[64, 128, 256]).to(device)
checkpoint = torch.load('../training/models/tcn_fuel_best.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ONNX export
dummy_input = torch.randn(1, 60, 11).to(device)
torch.onnx.export(
    model,
    dummy_input,
    '../models/tcn_fuel_prediction.onnx',
    export_params=True,
    opset_version=13,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
)

print('âœ… TCN ONNX export complete: tcn_fuel_prediction.onnx')

# ëª¨ë¸ í¬ê¸° í™•ì¸
import os
size_mb = os.path.getsize('../models/tcn_fuel_prediction.onnx') / (1024**2)
print(f'   Model size: {size_mb:.2f} MB (target: <4MB)')
"
```

**LSTM-AE ëª¨ë¸ ë³€í™˜**:
```python
python -c "
import torch
import sys
sys.path.append('..')
from training.train_lstm_ae import LSTM_Autoencoder

# ëª¨ë¸ ë¡œë“œ
device = torch.device('cuda')
model = LSTM_Autoencoder(input_dim=11, hidden_dim=128, num_layers=2, latent_dim=32).to(device)
checkpoint = torch.load('../training/models/lstm_ae_best.pth')
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# ONNX export
dummy_input = torch.randn(1, 60, 11).to(device)
torch.onnx.export(
    model,
    dummy_input,
    '../models/lstm_ae_anomaly_detection.onnx',
    export_params=True,
    opset_version=13,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output']
)

print('âœ… LSTM-AE ONNX export complete: lstm_ae_anomaly_detection.onnx')

# ëª¨ë¸ í¬ê¸° í™•ì¸
import os
size_mb = os.path.getsize('../models/lstm_ae_anomaly_detection.onnx') / (1024**2)
print(f'   Model size: {size_mb:.2f} MB (target: <3MB)')

# Threshold ì €ì¥
threshold = checkpoint['threshold']
print(f'   Anomaly threshold: {threshold:.6f}')
"
```

---

### Step 4.2: INT8 Quantization (ì„ íƒ, ê³ ê¸‰)

```python
# ì–‘ìí™”ë¡œ ëª¨ë¸ í¬ê¸° 50-75% ê°ì†Œ
# (ONNX Runtime quantization)

python -c "
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

# TCN quantization
model_fp32 = '../models/tcn_fuel_prediction.onnx'
model_quant = '../models/tcn_fuel_prediction_int8.onnx'

quantize_dynamic(
    model_fp32,
    model_quant,
    weight_type=QuantType.QUInt8
)

import os
size_before = os.path.getsize(model_fp32) / (1024**2)
size_after = os.path.getsize(model_quant) / (1024**2)
reduction = (1 - size_after/size_before) * 100

print(f'âœ… TCN INT8 quantization complete')
print(f'   Before: {size_before:.2f} MB')
print(f'   After:  {size_after:.2f} MB')
print(f'   Reduction: {reduction:.1f}%')

# LSTM-AE quantization (ë™ì¼í•œ ë°©ë²•)
# ...
"
```

---

### Step 4.3: ëª¨ë¸ ê²€ì¦

```bash
cd d:\edgeai\edgeai-repo\ai-models\conversion

# ONNX ëª¨ë¸ ê²€ì¦
python -c "
import onnx

# TCN ê²€ì¦
tcn_model = onnx.load('../models/tcn_fuel_prediction.onnx')
onnx.checker.check_model(tcn_model)
print('âœ… TCN ONNX model is valid')

# LSTM-AE ê²€ì¦
lstm_model = onnx.load('../models/lstm_ae_anomaly_detection.onnx')
onnx.checker.check_model(lstm_model)
print('âœ… LSTM-AE ONNX model is valid')

# ì „ì²´ ëª¨ë¸ í¬ê¸° í™•ì¸
import os
tcn_size = os.path.getsize('../models/tcn_fuel_prediction.onnx') / (1024**2)
lstm_size = os.path.getsize('../models/lstm_ae_anomaly_detection.onnx') / (1024**2)
lightgbm_size = 0.012  # 12KB (ì´ë¯¸ í•™ìŠµ ì™„ë£Œ)

total_size = tcn_size + lstm_size + lightgbm_size
print(f'')
print(f'ğŸ“Š Total Model Size:')
print(f'   TCN:      {tcn_size:.2f} MB')
print(f'   LSTM-AE:  {lstm_size:.2f} MB')
print(f'   LightGBM: {lightgbm_size:.3f} MB')
print(f'   Total:    {total_size:.2f} MB (target: <14MB)')

if total_size < 14:
    print('âœ… Within budget!')
else:
    print('âš ï¸ Exceeds budget, apply INT8 quantization')
"
```

---

## 5ï¸âƒ£ Android í†µí•© (30ë¶„-1ì‹œê°„)

### Step 5.1: ONNX ëª¨ë¸ì„ Androidì— ë³µì‚¬

```bash
# Windows:
cd d:\edgeai
xcopy /Y edgeai-repo\ai-models\models\*.onnx edgeai-repo\android-dtg\app\src\main\assets\models\

# Linux:
cd ~/edgeai
cp edgeai-repo/ai-models/models/*.onnx edgeai-repo/android-dtg/app/src/main/assets/models/

# ë³µì‚¬ í™•ì¸
dir edgeai-repo\android-dtg\app\src\main\assets\models\
# ë˜ëŠ” Linux: ls -lh edgeai-repo/android-dtg/app/src/main/assets/models/

# ì˜ˆìƒ ì¶œë ¥:
# tcn_fuel_prediction.onnx              (3.2 MB)
# lstm_ae_anomaly_detection.onnx        (2.1 MB)
# lightgbm_behavior_model.txt           (12 KB)
```

---

### Step 5.2: Android APK ë¹Œë“œ

```bash
cd d:\edgeai\edgeai-repo\android-dtg

# Gradle ë¹Œë“œ (Debug APK)
# Windows:
gradlew.bat assembleDebug

# Linux:
./gradlew assembleDebug

# ë¹Œë“œ ì‹œê°„: ì•½ 5-10ë¶„ (ì²« ë¹Œë“œëŠ” ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)

# ì˜ˆìƒ ì¶œë ¥:
# > Task :app:compileDebugKotlin
# > Task :app:mergeDebugAssets
# > Task :app:packageDebug
# BUILD SUCCESSFUL in 8m 23s
# 156 actionable tasks: 156 executed

# APK ìœ„ì¹˜ í™•ì¸
dir app\build\outputs\apk\debug\app-debug.apk
# ë˜ëŠ” Linux: ls -lh app/build/outputs/apk/debug/app-debug.apk

# ì˜ˆìƒ ì¶œë ¥:
# app-debug.apk  (ì•½ 15-20 MB)
```

**ë¹Œë“œ ì‹¤íŒ¨ ì‹œ ë¬¸ì œ í•´ê²°**:
```bash
# Gradle ìºì‹œ ì •ë¦¬
gradlew.bat clean
# ë˜ëŠ”: ./gradlew clean

# ë‹¤ì‹œ ë¹Œë“œ
gradlew.bat assembleDebug --stacktrace
```

---

### Step 5.3: ë””ë°”ì´ìŠ¤ì— ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸

```bash
# Android ë””ë°”ì´ìŠ¤ USB ì—°ê²° (USB ë””ë²„ê¹… í™œì„±í™” í•„ìš”)

# ADB ë””ë°”ì´ìŠ¤ í™•ì¸
adb devices

# ì˜ˆìƒ ì¶œë ¥:
# List of devices attached
# ABC123456789    device

# APK ì„¤ì¹˜
adb install -r app\build\outputs\apk\debug\app-debug.apk
# ë˜ëŠ” Linux: adb install -r app/build/outputs/apk/debug/app-debug.apk

# ì˜ˆìƒ ì¶œë ¥:
# Performing Streamed Install
# Success

# ì•± ì‹¤í–‰
adb shell am start -n com.glec.dtg/.MainActivity

# ë¡œê·¸ í™•ì¸
adb logcat -s DTG:* EdgeAI:* ONNX:*

# ì˜ˆìƒ ë¡œê·¸:
# DTG     : EdgeAI models loaded successfully
# EdgeAI  : TCN inference: 23.4ms
# EdgeAI  : LSTM-AE inference: 31.2ms
# EdgeAI  : LightGBM inference: 0.064ms
# DTG     : Total inference time: 54.7ms (target: <50ms) âš ï¸
```

---

## 6ï¸âƒ£ ì„±ëŠ¥ ê²€ì¦

### Step 6.1: ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì •

```python
cd d:\edgeai\edgeai-repo\tests

# ì¶”ë¡  ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (deviceì—ì„œ)
adb shell "am instrument -w -e class com.glec.dtg.test.InferencePerformanceTest com.glec.dtg.test/androidx.test.runner.AndroidJUnitRunner"

# ì˜ˆìƒ ì¶œë ¥:
# InferencePerformanceTest:
# - TCN inference: 23.4ms (avg), 25.6ms (P95) âœ… <25ms
# - LSTM-AE inference: 31.2ms (avg), 34.8ms (P95) âœ… <35ms
# - LightGBM inference: 0.064ms (avg) âœ… <15ms
# - Total (parallel): 31.5ms (avg) âœ… <50ms
# - Model size: 5.3 MB + 2.1 MB + 0.012 MB = 7.4 MB âœ… <14MB
```

---

### Step 6.2: ì •í™•ë„ ê²€ì¦

```python
cd d:\edgeai\edgeai-repo\ai-models\training

# Test datasetìœ¼ë¡œ ì •í™•ë„ ê²€ì¦
python -c "
import torch
import pandas as pd
import numpy as np
from train_tcn import TCN
from train_lstm_ae import LSTM_Autoencoder
from sklearn.metrics import r2_score, f1_score

device = torch.device('cuda')

# TCN ì •í™•ë„
print('ğŸ” TCN Fuel Prediction ì •í™•ë„:')
tcn_model = TCN(input_dim=11, output_dim=1, num_channels=[64, 128, 256]).to(device)
tcn_checkpoint = torch.load('models/tcn_fuel_best.pth')
tcn_model.load_state_dict(tcn_checkpoint['model_state_dict'])
tcn_model.eval()

# Test data ë¡œë“œ
test_df = pd.read_csv('../datasets/test.csv')
# (ë°ì´í„° ì „ì²˜ë¦¬ ë° í‰ê°€ ì½”ë“œ...)

print(f'   RÂ² Score: {tcn_checkpoint[\"r2_score\"]:.4f} (target: >0.85)')
print(f'   Val Loss: {tcn_checkpoint[\"val_loss\"]:.4f}')
print(f'   âœ… TCN meets target!' if tcn_checkpoint['r2_score'] > 0.85 else 'âš ï¸ Below target')

# LSTM-AE ì •í™•ë„
print('')
print('ğŸ” LSTM-AE Anomaly Detection ì •í™•ë„:')
lstm_model = LSTM_Autoencoder(input_dim=11, hidden_dim=128, num_layers=2, latent_dim=32).to(device)
lstm_checkpoint = torch.load('models/lstm_ae_best.pth')
lstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])
lstm_model.eval()

print(f'   F1-Score: {lstm_checkpoint[\"f1_score\"]:.4f} (target: >0.85)')
print(f'   Val Loss: {lstm_checkpoint[\"val_loss\"]:.4f}')
print(f'   Threshold: {lstm_checkpoint[\"threshold\"]:.6f}')
print(f'   âœ… LSTM-AE meets target!' if lstm_checkpoint['f1_score'] > 0.85 else 'âš ï¸ Below target')

# LightGBM (ì´ë¯¸ 99.54%)
print('')
print('ğŸ” LightGBM Behavior Classification:')
print(f'   Accuracy: 99.54% (target: >90%) âœ…')
print(f'   Latency: 0.064ms (target: <15ms) âœ…')
"
```

---

## 7ï¸âƒ£ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… í™˜ê²½ êµ¬ì¶•
- [ ] NVIDIA GPU ì‘ë™ í™•ì¸ (nvidia-smi)
- [ ] Miniconda ì„¤ì¹˜ ë° í™˜ê²½ ìƒì„±
- [ ] PyTorch CUDA ì‘ë™ í™•ì¸ (torch.cuda.is_available())
- [ ] ì˜ì¡´ì„± ì„¤ì¹˜ ì™„ë£Œ (requirements.txt)
- [ ] í…ŒìŠ¤íŠ¸ 159/159 passing

### âœ… ë°ì´í„° ìƒì„±
- [ ] train.csv ìƒì„± (anomaly_ratio=0%)
- [ ] val.csv ìƒì„± (anomaly_ratio=10%)
- [ ] test.csv ìƒì„± (anomaly_ratio=10%)
- [ ] ë°ì´í„° í’ˆì§ˆ ê²€ì¦ (no NaN, valid ranges)

### âœ… ëª¨ë¸ í•™ìŠµ
- [ ] TCN í•™ìŠµ ì™„ë£Œ (RÂ² > 0.85)
- [ ] LSTM-AE í•™ìŠµ ì™„ë£Œ (F1 > 0.85)
- [ ] LightGBM ì¬í•™ìŠµ (ì„ íƒ, ì´ë¯¸ 99.54%)
- [ ] í•™ìŠµ ë¡œê·¸ ì €ì¥ (models/*.pth)

### âœ… ëª¨ë¸ ìµœì í™”
- [ ] TCN ONNX ë³€í™˜
- [ ] LSTM-AE ONNX ë³€í™˜
- [ ] ëª¨ë¸ í¬ê¸° ê²€ì¦ (<14MB total)
- [ ] INT8 quantization (ì„ íƒ)

### âœ… Android í†µí•©
- [ ] ONNX ëª¨ë¸ ë³µì‚¬ (assets/models/)
- [ ] APK ë¹Œë“œ ì„±ê³µ
- [ ] ë””ë°”ì´ìŠ¤ ì„¤ì¹˜ ë° ì‹¤í–‰
- [ ] ì¶”ë¡  ì„±ëŠ¥ ê²€ì¦ (<50ms)

### âœ… ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±
- [ ] TCN: <4MB, <25ms, >85% RÂ²
- [ ] LSTM-AE: <3MB, <35ms, >85% F1
- [ ] LightGBM: <10MB, <15ms, >90% Acc
- [ ] Total: <14MB, <50ms parallel

---

## ğŸ‰ ì„±ê³µ ê¸°ì¤€

**âœ… ìµœì†Œ ì„±ê³µ** (MVP):
- TCN í•™ìŠµ ì™„ë£Œ, RÂ² > 0.80
- LSTM-AE í•™ìŠµ ì™„ë£Œ, F1 > 0.80
- APK ë¹Œë“œ ì„±ê³µ, ë””ë°”ì´ìŠ¤ì—ì„œ ì‹¤í–‰

**âœ… ëª©í‘œ ë‹¬ì„±** (Production-Ready):
- TCN RÂ² > 0.85, ì¶”ë¡  <25ms
- LSTM-AE F1 > 0.85, ì¶”ë¡  <35ms
- Total ëª¨ë¸ í¬ê¸° <14MB
- Total ì¶”ë¡  ì‹œê°„ <50ms (parallel)

**ğŸ† ì™„ë²½í•œ ì„±ê³µ** (World-Class):
- TCN RÂ² > 0.90
- LSTM-AE F1 > 0.90
- Total ì¶”ë¡  ì‹œê°„ <30ms
- Device ì „ë ¥ ì†Œëª¨ <2W

---

## ğŸ“Š ì˜ˆìƒ íƒ€ì„ë¼ì¸

| ë‹¨ê³„ | ì‘ì—… | ì˜ˆìƒ ì‹œê°„ | ëˆ„ì  ì‹œê°„ |
|------|------|-----------|-----------|
| 1 | í™˜ê²½ êµ¬ì¶• | 30ë¶„-1ì‹œê°„ | 1ì‹œê°„ |
| 2 | ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸) | 5-10ë¶„ | 1ì‹œê°„ 10ë¶„ |
| 3 | TCN í•™ìŠµ (í…ŒìŠ¤íŠ¸) | 15-30ë¶„ | 1ì‹œê°„ 40ë¶„ |
| 4 | LSTM-AE í•™ìŠµ (í…ŒìŠ¤íŠ¸) | 15-30ë¶„ | 2ì‹œê°„ 10ë¶„ |
| 5 | ê²€ì¦ ë° ì¡°ì • | 10-20ë¶„ | 2ì‹œê°„ 30ë¶„ |
| 6 | ë°ì´í„° ì¬ìƒì„± (production) | 20-30ë¶„ | 3ì‹œê°„ |
| 7 | TCN ì¬í•™ìŠµ (production) | 2-4ì‹œê°„ | 6ì‹œê°„ |
| 8 | LSTM-AE ì¬í•™ìŠµ (production) | 2-4ì‹œê°„ | 9ì‹œê°„ |
| 9 | ONNX ë³€í™˜ | 10-20ë¶„ | 9ì‹œê°„ 20ë¶„ |
| 10 | Android í†µí•© | 30ë¶„-1ì‹œê°„ | 10ì‹œê°„ |
| **í•©ê³„** | | **6-10ì‹œê°„** | |

**ê¶Œì¥ ì¼ì •**:
- **Day 1 (2-3ì‹œê°„)**: í™˜ê²½ êµ¬ì¶• + í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ + í…ŒìŠ¤íŠ¸ í•™ìŠµ
- **Day 2 (5-7ì‹œê°„)**: Production ë°ì´í„°ì…‹ + ì „ì²´ í•™ìŠµ + ìµœì í™”
- **Day 3 (1-2ì‹œê°„)**: Android í†µí•© + ê²€ì¦

---

## ğŸš¨ ë¬¸ì œ í•´ê²° (Troubleshooting)

### Issue 1: CUDA out of memory

**ì¦ìƒ**: `RuntimeError: CUDA out of memory`

**í•´ê²°**:
```bash
# Batch size ì¤„ì´ê¸°
python train_tcn.py --batch-size 32  # 64 â†’ 32

# ë˜ëŠ” ë” ì‘ê²Œ
python train_tcn.py --batch-size 16  # 64 â†’ 16
```

---

### Issue 2: í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¼

**ì¦ìƒ**: Epochë‹¹ 5ë¶„ ì´ìƒ ì†Œìš”

**í•´ê²°**:
```python
# train_tcn.py íŒŒì¼ ìˆ˜ì •
# DataLoader num_workers ì¦ê°€
DataLoader(..., num_workers=4)  # 0 â†’ 4

# ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ì¦ê°€ (VRAM ì—¬ìœ  ìˆëŠ” ê²½ìš°)
python train_tcn.py --batch-size 128  # 64 â†’ 128
```

---

### Issue 3: ì •í™•ë„ê°€ ëª©í‘œì— ë¯¸ë‹¬

**ì¦ìƒ**: RÂ² < 0.85 ë˜ëŠ” F1 < 0.85

**í•´ê²°**:
```bash
# ë” ë§ì€ epochìœ¼ë¡œ í•™ìŠµ
python train_tcn.py --epochs 200  # 100 â†’ 200

# Learning rate ì¡°ì •
# config.yaml ìˆ˜ì •: learning_rate: 0.0005  # 0.001 â†’ 0.0005

# ë” ë§ì€ ë°ì´í„° ìƒì„± (20,000 ìƒ˜í”Œ)
# data_generation ìŠ¤í¬ë¦½íŠ¸ì—ì„œ num_samples ì¦ê°€
```

---

### Issue 4: Android APK ë¹Œë“œ ì‹¤íŒ¨

**ì¦ìƒ**: Gradle ë¹Œë“œ ì—ëŸ¬

**í•´ê²°**:
```bash
# Gradle ìºì‹œ ì •ë¦¬
gradlew.bat clean
del /s /q .gradle
del /s /q build

# ë‹¤ì‹œ ë¹Œë“œ
gradlew.bat assembleDebug --stacktrace --info

# Java ë²„ì „ í™•ì¸ (JDK 17 í•„ìš”)
java -version
```

---

## ğŸ“ ì§€ì› ë° ë¬¸ì˜

**ë¬¸ì œ í•´ê²° ë¦¬ì†ŒìŠ¤**:
1. **PRE_EXECUTION_VALIDATION_CHECKLIST.md**: 32ê°€ì§€ í™˜ê²½ ê²€ì¦ í•­ëª©
2. **GPU_TRAINING_EXECUTION_GUIDE.md**: ìƒì„¸í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œ
3. **CURSOR.md**: Cursor AIë¥¼ ìœ„í•œ ì‹¤í–‰ ê°€ì´ë“œ
4. **CTO_COMPREHENSIVE_ANALYSIS_REPORT.md**: í”„ë¡œì íŠ¸ ìƒíƒœ ë¶„ì„

**GitHub Issues**: https://github.com/glecdev/edgeai/issues

---

## ğŸ“ í•™ìŠµ ì™„ë£Œ í›„

### ë‹¤ìŒ ë‹¨ê³„

1. **ì‹¤ì œ ì°¨ëŸ‰ í…ŒìŠ¤íŠ¸**:
   - STM32 CAN bus ì—°ê²°
   - ì‹¤ì œ ì°¨ëŸ‰ ë°ì´í„°ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
   - Edge í™˜ê²½ì—ì„œ ì„±ëŠ¥ ê²€ì¦

2. **ëª¨ë¸ ê°œì„ **:
   - ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
   - Hyperparameter tuning (grid search)
   - Ensemble ëª¨ë¸ ì‹œë„

3. **Production ë°°í¬**:
   - OTA ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ
   - Fleet AI í”Œë«í¼ ì—°ë™
   - ëª¨ë‹ˆí„°ë§ ë° A/B í…ŒìŠ¤íŠ¸

---

**ì¤€ë¹„ëë‚˜ìš”? ì‹œì‘í•˜ì„¸ìš”! ğŸš€**

```bash
# í™˜ê²½ í™œì„±í™”
conda activate dtg-ai

# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd d:\edgeai\edgeai-repo

# Step 1ë¶€í„° ì‹œì‘!
nvidia-smi
```

**ì¢‹ì€ í•™ìŠµ ë˜ì„¸ìš”! ğŸ’ª**
