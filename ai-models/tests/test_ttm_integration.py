"""
Unit tests for IBM TTM-r2 integration with GLEC DTG Edge AI

Tests:
1. Model loading and configuration
2. Input/output shape validation
3. Zero-shot inference
4. Few-shot fine-tuning compatibility
5. Performance benchmarks (latency, memory)
"""

import unittest
import sys
from pathlib import Path
import tempfile
import time

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import torch
    import numpy as np
    from transformers import AutoModel, AutoConfig
    DEPENDENCIES_AVAILABLE = True
except ImportError:
    DEPENDENCIES_AVAILABLE = False


@unittest.skipUnless(DEPENDENCIES_AVAILABLE, "torch and transformers required")
class TestTTMr2Integration(unittest.TestCase):
    """Test IBM TTM-r2 model integration"""

    MODEL_NAME = "ibm-granite/granite-timeseries-ttm-r2"

    @classmethod
    def setUpClass(cls):
        """Load model once for all tests"""
        print("\nüîß Loading TTM-r2 model for testing...")
        try:
            cls.model = AutoModel.from_pretrained(cls.MODEL_NAME)
            cls.config = AutoConfig.from_pretrained(cls.MODEL_NAME)
            cls.model.eval()
            print("‚úÖ Model loaded successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load model: {e}")
            cls.model = None
            cls.config = None

    def test_model_loaded(self):
        """Test model and config loaded successfully"""
        self.assertIsNotNone(self.model, "Model should be loaded")
        self.assertIsNotNone(self.config, "Config should be loaded")

    def test_model_parameters_count(self):
        """Test model has reasonable parameter count for edge deployment"""
        if self.model is None:
            self.skipTest("Model not loaded")

        param_count = sum(p.numel() for p in self.model.parameters())
        param_mb = param_count * 4 / (1024 ** 2)  # 4 bytes per float32

        print(f"\nüìä Model size: {param_count:,} parameters ({param_mb:.1f} MB FP32)")

        # TTM-r2 should be < 10M parameters for "Tiny" designation
        self.assertLess(
            param_count,
            20_000_000,  # 20M max
            "Model should be < 20M parameters for edge deployment"
        )

    def test_input_shape_validation(self):
        """Test model accepts vehicle time series input shape"""
        if self.model is None:
            self.skipTest("Model not loaded")

        # GLEC DTG input: (batch, lookback_seconds, features)
        # Features: speed, rpm, throttle, brake, fuel, coolant, accel_xyz, steering
        batch_size = 1
        lookback = 60  # 60 seconds at 1Hz
        num_features = 10  # Selected vehicle features

        test_input = torch.randn(batch_size, lookback, num_features)

        try:
            with torch.no_grad():
                output = self.model(
                    past_values=test_input,
                    freq_token=0,  # 1Hz sampling
                )
            self.assertIsNotNone(output, "Model should return output")
            print(f"‚úÖ Input shape validated: {test_input.shape}")
            print(f"   Output shape: {output.prediction_outputs.shape}")

        except Exception as e:
            self.fail(f"Model inference failed: {e}")

    def test_zero_shot_inference(self):
        """Test zero-shot forecasting without fine-tuning"""
        if self.model is None:
            self.skipTest("Model not loaded")

        # Simulate realistic vehicle data (normalized 0-1)
        vehicle_data = torch.tensor([
            # Last 60 seconds of driving
            [[0.5, 0.6, 0.3, 0.0, 0.9, 0.5, 0.1, 0.0, 0.0, 0.0]],  # Normal driving
        ], dtype=torch.float32).repeat(1, 60, 1)  # Repeat for 60 timesteps

        with torch.no_grad():
            output = self.model(
                past_values=vehicle_data,
                freq_token=0,
            )

        # Check output is finite (no NaN/Inf)
        self.assertTrue(
            torch.isfinite(output.prediction_outputs).all(),
            "Output should be finite (no NaN or Inf)"
        )

        print(f"‚úÖ Zero-shot inference successful")
        print(f"   Output range: [{output.prediction_outputs.min():.4f}, {output.prediction_outputs.max():.4f}]")

    def test_inference_latency(self):
        """Test inference latency meets edge AI requirements (<50ms)"""
        if self.model is None:
            self.skipTest("Model not loaded")

        test_input = torch.randn(1, 60, 10)

        # Warmup
        with torch.no_grad():
            _ = self.model(past_values=test_input, freq_token=0)

        # Benchmark
        num_runs = 100
        latencies = []

        for _ in range(num_runs):
            start = time.perf_counter()
            with torch.no_grad():
                _ = self.model(past_values=test_input, freq_token=0)
            latency_ms = (time.perf_counter() - start) * 1000
            latencies.append(latency_ms)

        mean_latency = np.mean(latencies)
        p95_latency = np.percentile(latencies, 95)

        print(f"\n‚è±Ô∏è Inference latency (CPU):")
        print(f"   Mean: {mean_latency:.2f} ms")
        print(f"   P95:  {p95_latency:.2f} ms")

        # Warning if > 50ms (target for edge), but don't fail
        # (actual edge deployment will use quantization + hardware acceleration)
        if p95_latency > 50:
            print(f"‚ö†Ô∏è P95 latency ({p95_latency:.2f}ms) > 50ms target")
            print(f"   ‚Üí Apply INT8 quantization for edge deployment")

    def test_batch_inference(self):
        """Test model supports batch inference for efficiency"""
        if self.model is None:
            self.skipTest("Model not loaded")

        batch_sizes = [1, 4, 8]

        for batch_size in batch_sizes:
            test_input = torch.randn(batch_size, 60, 10)

            with torch.no_grad():
                output = self.model(past_values=test_input, freq_token=0)

            expected_batch = batch_size
            actual_batch = output.prediction_outputs.shape[0]

            self.assertEqual(
                actual_batch,
                expected_batch,
                f"Batch size should be preserved: {expected_batch} != {actual_batch}"
            )

        print(f"‚úÖ Batch inference validated for sizes: {batch_sizes}")

    def test_model_deterministic(self):
        """Test model produces deterministic outputs in eval mode"""
        if self.model is None:
            self.skipTest("Model not loaded")

        self.model.eval()
        test_input = torch.randn(1, 60, 10)

        with torch.no_grad():
            output1 = self.model(past_values=test_input, freq_token=0)
            output2 = self.model(past_values=test_input, freq_token=0)

        # Outputs should be identical in eval mode
        torch.testing.assert_close(
            output1.prediction_outputs,
            output2.prediction_outputs,
            msg="Model should be deterministic in eval mode"
        )

        print("‚úÖ Model is deterministic (eval mode)")

    def test_config_attributes(self):
        """Test model config has expected attributes"""
        if self.config is None:
            self.skipTest("Config not loaded")

        # Check essential config attributes
        self.assertTrue(
            hasattr(self.config, 'model_type'),
            "Config should have model_type"
        )

        print(f"\nüìã Model config:")
        print(f"   Model type: {self.config.model_type}")

        if hasattr(self.config, 'd_model'):
            print(f"   Hidden dim: {self.config.d_model}")
        if hasattr(self.config, 'num_layers'):
            print(f"   Num layers: {self.config.num_layers}")


class TestTTMr2DataPreprocessing(unittest.TestCase):
    """Test data preprocessing for TTM-r2"""

    def test_feature_extraction(self):
        """Test extracting relevant features from GLEC CAN data"""
        # Mock CAN data structure
        mock_can_data = {
            'vehicle_speed': 65.5,
            'engine_rpm': 2500.0,
            'throttle_position': 45.0,
            'brake_pressure': 0.0,
            'fuel_level': 75.5,
            'coolant_temp': 92.0,
            'acceleration_x': 0.5,
            'acceleration_y': -0.1,
            'acceleration_z': 0.0,
            'steering_angle': 5.0,
        }

        # Features should match model input (10 features)
        features = list(mock_can_data.values())
        self.assertEqual(len(features), 10, "Should have 10 features")

        # All features should be numeric
        self.assertTrue(
            all(isinstance(f, (int, float)) for f in features),
            "All features should be numeric"
        )

    def test_normalization(self):
        """Test feature normalization for model input"""
        # Mock raw features (different scales)
        raw_features = np.array([
            [120.0, 3500.0, 80.0, 0.0, 50.0, 95.0, 2.0, -0.5, 0.0, -10.0]
        ])

        # Normalization ranges (from synthetic data analysis)
        ranges = {
            'speed': (0, 200),
            'rpm': (800, 6500),
            'throttle': (0, 100),
            'brake': (0, 100),
            'fuel': (0, 100),
            'coolant': (40, 120),
            'accel_x': (-10, 10),
            'accel_y': (-10, 10),
            'accel_z': (-10, 10),
            'steering': (-45, 45),
        }

        # Min-max normalization to [0, 1]
        normalized = np.zeros_like(raw_features)
        for i, (min_val, max_val) in enumerate(ranges.values()):
            normalized[0, i] = (raw_features[0, i] - min_val) / (max_val - min_val)

        # Check normalized values are in [0, 1]
        self.assertTrue(
            np.all((normalized >= 0) & (normalized <= 1)),
            "Normalized features should be in [0, 1]"
        )

        print(f"‚úÖ Normalization validated")
        print(f"   Raw:        {raw_features[0, :3]}")
        print(f"   Normalized: {normalized[0, :3]}")


def suite():
    """Create test suite"""
    suite = unittest.TestSuite()
    suite.addTest(unittest.makeSuite(TestTTMr2Integration))
    suite.addTest(unittest.makeSuite(TestTTMr2DataPreprocessing))
    return suite


if __name__ == '__main__':
    if not DEPENDENCIES_AVAILABLE:
        print("‚ö†Ô∏è Skipping tests: torch and transformers not installed")
        print("   Install: pip install torch transformers")
        sys.exit(0)

    unittest.main(verbosity=2)
