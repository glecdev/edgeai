# ì—£ì§€ AI ëª¨ë¸ í¬ê´„ì  ë¶„ì„: ì˜¤í”ˆì†ŒìŠ¤/ì˜¤í”ˆëª¨ë¸ ì¡°í•© ë¹„êµ

**ì‘ì„±ì¼**: 2025-01-09
**ëª©ì **: ì°¨ëŸ‰ í…”ë ˆë§¤í‹±ìŠ¤ ì—£ì§€ AI êµ¬í˜„ì„ ìœ„í•œ ìµœì  ëª¨ë¸ ì¡°í•© ì„ ì •
**ë¶„ì„ ì±„ë„**: Hugging Face, GitHub, Unsloth, í•™ìˆ  ë…¼ë¬¸, ì‚°ì—… ì‚¬ë¡€ (Samsara)

---

## ğŸ“‹ ëª©ì°¨

1. [Executive Summary](#executive-summary)
2. [ì‚°ì—… ë²¤ì¹˜ë§ˆí¬: Samsara ì‚¬ë¡€](#ì‚°ì—…-ë²¤ì¹˜ë§ˆí¬-samsara-ì‚¬ë¡€)
3. [ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„](#ì˜¤í”ˆì†ŒìŠ¤-ëª¨ë¸-ì¹´í…Œê³ ë¦¬ë³„-ë¶„ì„)
4. [ëª¨ë¸ ì¡°í•© ì‹œë‚˜ë¦¬ì˜¤](#ëª¨ë¸-ì¡°í•©-ì‹œë‚˜ë¦¬ì˜¤)
5. [ìµœì¢… ê¶Œì¥ ì¡°í•©](#ìµœì¢…-ê¶Œì¥-ì¡°í•©)
6. [êµ¬í˜„ ë¡œë“œë§µ](#êµ¬í˜„-ë¡œë“œë§µ)

---

## ğŸ¯ Executive Summary

### í•µì‹¬ ë°œê²¬

**1. ì—£ì§€ LLMì€ ì•ˆì „ í•„ìˆ˜ í…”ë ˆë§¤í‹±ìŠ¤ì— ë¶€ì í•©**
- âŒ í™˜ê°(Hallucination) ë¬¸ì œ â†’ ì•ˆì „ ì‹œìŠ¤í…œì— ì¹˜ëª…ì 
- âŒ ë†’ì€ ì§€ì—°ì‹œê°„ (100ms+) â†’ ì‹¤ì‹œê°„ ê²½ê³  ë¶ˆê°€ëŠ¥
- âŒ ë†’ì€ ì „ë ¥ ì†Œëª¨ (5W+) â†’ ì°¨ëŸ‰ ë°°í„°ë¦¬ ë¶€ë‹´
- âŒ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„± â†’ ê²°ì •ë¡ ì  í–‰ë™ í•„ìš”í•œ ì‹œìŠ¤í…œì— ë¶€ì í•©

**2. Task-Specific MLì´ ìµœì  (Samsara ê²€ì¦)**
- âœ… ë¹ ë¥¸ ì‘ë‹µ (<50ms)
- âœ… ê²°ì •ë¡ ì /ì‹ ë¢°ì„±
- âœ… ì €ì „ë ¥ (<2W)
- âœ… ëª…í™•í•œ ëª©ì ë³„ ìµœì í™”

**3. í•˜ì´ë¸Œë¦¬ë“œ ì•„í‚¤í…ì²˜ ê¶Œì¥**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Core Safety Layer (Edge AI - Task-Specific)    â”‚
â”‚ âœ… ì—°ë£Œ ì˜ˆì¸¡: TCN/TTM                           â”‚
â”‚ âœ… ì´ìƒ íƒì§€: LSTM-AE/Anomalib                  â”‚
â”‚ âœ… í–‰ë™ ë¶„ë¥˜: LightGBM/Random Forest            â”‚
â”‚ ëª©í‘œ: <50ms, <14MB, <2W, ê²°ì •ë¡ ì               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ (ë³„ë„ í”„ë¡œì„¸ìŠ¤)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UX Enhancement Layer (Optional)                 â”‚
â”‚ ğŸ¤ ìŒì„± ë¹„ì„œ: Whisper tiny + Vosk              â”‚
â”‚ ğŸ’¬ ìì—°ì–´ ìƒí˜¸ì‘ìš©: ì œí•œì  ìš©ë„ë§Œ               â”‚
â”‚ âš ï¸  ì•ˆì „ ê¸°ëŠ¥ê³¼ ë¶„ë¦¬                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ­ ì‚°ì—… ë²¤ì¹˜ë§ˆí¬: Samsara ì‚¬ë¡€

### Samsara í˜„ì¬ ì ‘ê·¼ë²• (ìƒìš© ê²€ì¦)

**í•µì‹¬ ì „ëµ**: Task-Specific Vision ML

```
[ì°¨ëŸ‰ ì„¼ì„œ] â†’ [ê²½ëŸ‰ ë¹„ì „ ëª¨ë¸] â†’ [ì‹¤ì‹œê°„ ê²½ê³ ]
                 â†“
          - ì¡¸ìŒ ê°ì§€
          - ì°¨ì„  ì´íƒˆ
          - ì¶©ëŒ ê²½ê³ 
          - ê°ì²´ ê°ì§€

íŠ¹ì§•:
- ëª¨ë¸ í¬ê¸°: <10MB per task
- ì§€ì—°ì‹œê°„: <30ms
- ì „ë ¥: <1.5W
- ì‹ ë¢°ì„±: 99.9%+
```

### Samsara vs ì—£ì§€ LLM ë¹„êµ

| í•­ëª© | Samsara (Task-Specific) | ì—£ì§€ LLM | ìŠ¹ì |
|------|------------------------|---------|------|
| **íš¨ìœ¨ì„±/ì†ë„** | âœ… ë§¤ìš° ë¹ ë¦„ (<30ms) | âŒ ëŠë¦¼ (100ms+) | **Samsara** |
| **ì‹ ë¢°ì„±** | âœ… ê²°ì •ë¡ ì  | âŒ í™˜ê° ê°€ëŠ¥ | **Samsara** |
| **ìœ ì—°ì„±** | âš ï¸ íŠ¹ì • ì‘ì—…ë§Œ | âœ… ë²”ìš© | ì—£ì§€ LLM |
| **í•˜ë“œì›¨ì–´ ìš”êµ¬** | âœ… ì €ì „ë ¥ NPU | âŒ ê³ ì„±ëŠ¥ GPU | **Samsara** |
| **ì „ë ¥ ì†Œëª¨** | âœ… <1.5W | âŒ >5W | **Samsara** |
| **ì•ˆì „ ì í•©ì„±** | âœ… ì™„ë²½ | âŒ ë¶€ì í•© | **Samsara** |
| **ì£¼ìš” ìš©ë„** | ì‹¤ì‹œê°„ ì•ˆì „ ê²½ê³  | ìŒì„± ë¹„ì„œ/ëŒ€í™” | ê°ê° ë‹¤ë¦„ |

**ê²°ë¡ **: **ì•ˆì „ í•„ìˆ˜ ê¸°ëŠ¥ì—ëŠ” Task-Specific ML í•„ìˆ˜**, ì—£ì§€ LLMì€ ë¶€ê°€ ì„œë¹„ìŠ¤ ì „ìš©

---

## ğŸ”¬ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„

### Category 1: ì‹œê³„ì—´ ì˜ˆì¸¡ (Fuel Prediction)

#### Option 1-A: IBM Granite TTM (Tiny Time Mixer) â­ **ì¶”ì²œ**

**ì¶œì²˜**: Hugging Face `ibm-granite/granite-timeseries-ttm-r2`

**í•µì‹¬ ìŠ¤í™**:
```yaml
Parameters: 1M-10M (ìµœì†Œ 1Më¶€í„°)
Model Size: 4-40MB (ë¯¸ì–‘ìí™”)
Latency: 5-15ms (CPU)
Training: 1 GPU or laptop ê°€ëŠ¥
License: Apache 2.0
```

**ì¥ì **:
- âœ… **"Tiny" ì „ìš© ì„¤ê³„** (NeurIPS 2024 ì±„íƒ)
- âœ… **Zero-shot ì„±ëŠ¥ ìš°ìˆ˜** (ì¬í•™ìŠµ ì—†ì´ ì‚¬ìš© ê°€ëŠ¥)
- âœ… **Few-shot fine-tuning** (ì†ŒëŸ‰ ë°ì´í„°ë¡œ ê°œì„ )
- âœ… **ì—£ì§€ ìµœì í™”** (laptopì—ì„œ ì‹¤í–‰ ê°€ëŠ¥)
- âœ… **Billions-param ëª¨ë¸ ëŠ¥ê°€** (ì—¬ëŸ¬ ë²¤ì¹˜ë§ˆí¬ì—ì„œ)

**ë‹¨ì **:
- âš ï¸ ì‹œê³„ì—´ ì „ìš© (ë²”ìš©ì„± ì—†ìŒ, ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ì—ê² ì¥ì )
- âš ï¸ ìƒëŒ€ì  ì‹ ê·œ (2024ë…„, ê²€ì¦ í•„ìš”)

**ì ìš© ì˜ˆì‹œ**:
```python
from transformers import AutoModel
import torch

# Zero-shot ì‚¬ìš©
model = AutoModel.from_pretrained("ibm-granite/granite-timeseries-ttm-r2")
model.eval()

# ì…ë ¥: (batch, lookback_len, num_features)
x = torch.randn(1, 60, 10)  # 60ì´ˆ, 10ê°œ feature

# ì˜ˆì¸¡: ë‹¤ìŒ ì‹œì  ì—°ë£Œ ì†Œë¹„
with torch.no_grad():
    forecast = model(x, horizon=1)  # 1 step ahead

# TFLite ë³€í™˜ ìš©ì´
```

**vs í˜„ì¬ TCN**:

| í•­ëª© | TTM | TCN (Custom) | ë¹„ê³  |
|------|-----|--------------|------|
| íŒŒë¼ë¯¸í„° | 1M-10M | ~5M | TTM ë” ì‘ì„ ìˆ˜ ìˆìŒ |
| ì‚¬ì „ í•™ìŠµ | âœ… ìˆìŒ | âŒ ì—†ìŒ | TTM ìœ ë¦¬ |
| Zero-shot | âœ… ê°€ëŠ¥ | âŒ ë¶ˆê°€ | TTM ìœ ë¦¬ |
| ì»¤ìŠ¤í„°ë§ˆì´ì§• | âš ï¸ ì œí•œì  | âœ… ì™„ì „ ì œì–´ | TCN ìœ ë¦¬ |
| ì„±ìˆ™ë„ | âš ï¸ ì‹ ê·œ (2024) | âœ… ê²€ì¦ë¨ | TCN ìœ ë¦¬ |

**ê¶Œì¥**: **TTM + Custom TCN ë³‘ë ¬ í…ŒìŠ¤íŠ¸**

---

#### Option 1-B: Google TimesFM

**ì¶œì²˜**: Hugging Face `google/timesfm-1.0-200m`

**í•µì‹¬ ìŠ¤í™**:
```yaml
Parameters: 200M
Model Size: ~800MB (FP32)
Context Length: 512 time points
License: Apache 2.0
```

**í‰ê°€**:
- âŒ **ë„ˆë¬´ í¼** (200M params, >800MB)
- âŒ **ì—£ì§€ ë¶€ì í•©** (ëª©í‘œ <14MB ì´ˆê³¼)
- âœ… **ì„±ëŠ¥ ìš°ìˆ˜** (Google í’ˆì§ˆ)

**ê²°ë¡ **: âŒ **ì œì™¸** (í¬ê¸° ì´ˆê³¼)

---

#### Option 1-C: Custom TCN (í˜„ì¬ ì„¤ê³„)

**ìƒíƒœ**: âœ… ì´ë¯¸ êµ¬í˜„ë¨

**í•µì‹¬ ìŠ¤í™**:
```yaml
Parameters: ~5M
Model Size: 2-4MB (INT8)
Latency: 15-25ms
Architecture: 3-layer dilated causal convolution
```

**ì¥ì **:
- âœ… **ì™„ì „ ì œì–´ ê°€ëŠ¥**
- âœ… **ê²€ì¦ëœ ì•„í‚¤í…ì²˜**
- âœ… **í¬ê¸° ìµœì í™” ìš©ì´**
- âœ… **ì´ë¯¸ êµ¬í˜„ë¨**

**ë‹¨ì **:
- âš ï¸ ì‚¬ì „ í•™ìŠµ ì—†ìŒ (ì²˜ìŒë¶€í„° í•™ìŠµ)
- âš ï¸ ë°ì´í„° ì˜ì¡´ë„ ë†’ìŒ

**ê¶Œì¥**: âœ… **ìœ ì§€** (baseline)

---

### Category 2: ì´ìƒ íƒì§€ (Anomaly Detection)

#### Option 2-A: Intel/OpenVINO Anomalib â­ **ì¶”ì²œ**

**ì¶œì²˜**: GitHub `open-edge-platform/anomalib`

**í•µì‹¬ ìŠ¤í™**:
```yaml
Models:
  - PatchCore (sota)
  - FastFlow
  - PaDiM
  - LSTM-AE (ìš°ë¦¬ êµ¬í˜„ê³¼ ìœ ì‚¬)
Features:
  - Hyper-parameter optimization
  - Edge inference ready
  - ONNX/OpenVINO export
License: Apache 2.0
```

**ì¥ì **:
- âœ… **ì—£ì§€ ì „ìš© ì„¤ê³„** (Intel OpenVINO ìµœì í™”)
- âœ… **State-of-the-art ì•Œê³ ë¦¬ì¦˜** (PatchCore ë“±)
- âœ… **ì‹¤í—˜ ê´€ë¦¬ ë‚´ì¥** (MLflow í†µí•©)
- âœ… **ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”**
- âœ… **ì‚°ì—… ê²€ì¦** (Intel ì§€ì›)

**ë‹¨ì **:
- âš ï¸ ì£¼ë¡œ ì´ë¯¸ì§€ ê¸°ë°˜ (ì‹œê³„ì—´ì€ ë¶€ë¶„ì )
- âš ï¸ LSTM-AE êµ¬í˜„ì€ ìš°ë¦¬ì™€ ìœ ì‚¬

**ì ìš© ë°©ì•ˆ**:
```python
from anomalib.models import Patchcore
from anomalib.data import AnomalibDataModule

# ì‹œê³„ì—´ â†’ ì´ë¯¸ì§€ ë³€í™˜ (Gramian Angular Field)
# ë˜ëŠ” ì§ì ‘ LSTM-AE ì‚¬ìš©

model = Patchcore()
# ... í•™ìŠµ ë° ì—£ì§€ ë°°í¬
```

**vs í˜„ì¬ LSTM-AE**:

| í•­ëª© | Anomalib | LSTM-AE (Custom) | ë¹„ê³  |
|------|----------|------------------|------|
| ì•Œê³ ë¦¬ì¦˜ ë‹¤ì–‘ì„± | âœ… 10+ ì•Œê³ ë¦¬ì¦˜ | âŒ 1ê°œ | Anomalib ìœ ë¦¬ |
| ì—£ì§€ ìµœì í™” | âœ… ë‚´ì¥ | âš ï¸ ìˆ˜ë™ | Anomalib ìœ ë¦¬ |
| ì‹œê³„ì—´ ì „ìš©ì„± | âš ï¸ ì œí•œì  | âœ… ì™„ë²½ | LSTM-AE ìœ ë¦¬ |
| ì‹¤í—˜ ê´€ë¦¬ | âœ… ìë™í™” | âš ï¸ ìˆ˜ë™ | Anomalib ìœ ë¦¬ |

**ê¶Œì¥**: **LSTM-AE ìœ ì§€ + Anomalib PatchCore ì¶”ê°€ í…ŒìŠ¤íŠ¸**

---

#### Option 2-B: CAN Bus ì „ìš© ì˜¤í”ˆì†ŒìŠ¤

**ì¶œì²˜**: GitHub `nhorro/can-anomaly-detection`

**í•µì‹¬ ìŠ¤í™**:
```python
# LSTM + Autoencoder ì¡°í•©
# CAN ë²„ìŠ¤ íŠ¸ë˜í”½ ì „ìš©
# í•™ìˆ  ë…¼ë¬¸ ê¸°ë°˜

Model: LSTM-Autoencoder
Target: CAN intrusion detection
License: MIT
```

**í‰ê°€**:
- âœ… **CAN ë²„ìŠ¤ ì „ìš©**
- âœ… **í•™ìˆ  ê²€ì¦**
- âš ï¸ **ìš°ë¦¬ êµ¬í˜„ê³¼ ìœ ì‚¬** (ì¤‘ë³µ)

**ê²°ë¡ **: âš ï¸ **ì°¸ê³ ìš©** (ìš°ë¦¬ LSTM-AEì™€ ê±°ì˜ ë™ì¼)

---

#### Option 2-C: Transformer-based (AnomalyBERT, TranAD)

**ì¶œì²˜**: arXiv, Hugging Face

**í•µì‹¬ ìŠ¤í™**:
```yaml
AnomalyBERT:
  - Self-supervised transformer
  - Data degradation scheme

TranAD:
  - Deep transformer network
  - Attention-based sequence encoder

ë¬¸ì œì :
  - í¬ê¸°: 50M-100M+ parameters
  - ì§€ì—°: >100ms
  - ì „ë ¥: >3W
```

**í‰ê°€**:
- âŒ **ë„ˆë¬´ í¼** (ì—£ì§€ ë¶€ì í•©)
- âŒ **ë†’ì€ ì§€ì—°ì‹œê°„**
- âœ… **ì„±ëŠ¥ ìš°ìˆ˜** (ë²¤ì¹˜ë§ˆí¬ 1ìœ„)

**ê²°ë¡ **: âŒ **ì œì™¸** (í¬ê¸°/ì§€ì—° ì´ˆê³¼)

---

### Category 3: í–‰ë™ ë¶„ë¥˜ (Behavior Classification)

#### Option 3-A: LightGBM (í˜„ì¬ ì„¤ê³„) â­ **ìµœì **

**ì¶œì²˜**: Microsoft LightGBM (MIT License)

**í•µì‹¬ ìŠ¤í™**:
```yaml
Model Size: 5-10MB
Latency: 5-15ms (CPU)
Accuracy: 90-95%
Algorithm: Gradient Boosting Decision Tree
```

**ì¥ì **:
- âœ… **ê²€ì¦ëœ ì‚°ì—… í‘œì¤€**
- âœ… **ë§¤ìš° ë¹ ë¦„** (íŠ¸ë¦¬ ê¸°ë°˜)
- âœ… **í•´ì„ ê°€ëŠ¥** (feature importance)
- âœ… **ì—£ì§€ ìµœì ** (CPU ì „ìš© ê°€ëŠ¥)
- âœ… **Java ë„¤ì´í‹°ë¸Œ ì§€ì›** (Android)

**ë‹¨ì **:
- âš ï¸ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í•„ìš”

**ê¶Œì¥**: âœ… **ìœ ì§€** (ìµœì  ì„ íƒ)

---

#### Option 3-B: Random Forest

**í‰ê°€**:
```yaml
Pros:
  - í•´ì„ ê°€ëŠ¥
  - ê³¼ì í•© ë°©ì§€
  - ë³‘ë ¬í™” ìš©ì´

Cons:
  - LightGBMë³´ë‹¤ ëŠë¦¼
  - ëª¨ë¸ í¬ê¸° ë” í¼
```

**ê²°ë¡ **: âš ï¸ **ë°±ì—… ì˜µì…˜** (LightGBM ì‹¤íŒ¨ ì‹œ)

---

#### Option 3-C: Tiny Transformer (DistilBERT for IoT)

**ì¶œì²˜**: Nature Scientific Reports (2025)

**í•µì‹¬ ìŠ¤í™**:
```yaml
Model: DistilBERT (optimized)
Use Case: IoT attack classification
Parameters: ~66M (DistilBERT)
Accuracy: 95%+ (IoT intrusion)
```

**í‰ê°€**:
- âŒ **ì—¬ì „íˆ í¼** (66M params)
- âŒ **ë¶„ë¥˜ì— ì˜¤ë²„í‚¬** (ê°„ë‹¨í•œ ì‘ì—…)
- âœ… **IoT ê²€ì¦ë¨**

**ê²°ë¡ **: âŒ **ì œì™¸** (LightGBMì´ ì¶©ë¶„)

---

### Category 4: ëª¨ë¸ ì••ì¶•/ìµœì í™” ë„êµ¬

#### Option 4-A: Unsloth â­ **ì¶”ì²œ**

**ì¶œì²˜**: GitHub `unslothai/unsloth`

**í•µì‹¬ ê¸°ëŠ¥**:
```yaml
Quantization:
  - Dynamic 4-bit (1.58bitê¹Œì§€ ê°€ëŠ¥)
  - QAT (Quantization-Aware Training)
  - 70% ì •í™•ë„ ë³µêµ¬

Optimization:
  - 2x faster training
  - 70% less VRAM
  - ExecuTorch export (ëª¨ë°”ì¼)

Supported:
  - PyTorch â†’ GGUF
  - PyTorch â†’ ONNX
  - 4-bit/8-bit quantization
```

**ì ìš©**:
```python
from unsloth import FastLanguageModel

# í•™ìŠµ ì‹œ QAT ì ìš©
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="our-tcn-model",
    max_seq_length=60,
    dtype=None,
    load_in_4bit=True,  # 4-bit QAT
)

# í•™ìŠµ í›„ GGUF ë³€í™˜
model.save_pretrained_gguf("tcn_quantized", quantization_method="q4_k_m")

# ExecuTorchë¡œ ëª¨ë°”ì¼ ë°°í¬
model.export_to_executorch("tcn_mobile.pte")
```

**ì¥ì **:
- âœ… **ìµœì‹  ì–‘ìí™” ê¸°ìˆ ** (2024-2025)
- âœ… **ì •í™•ë„ ì†ì‹¤ ìµœì†Œ** (QATë¡œ 70% ë³µêµ¬)
- âœ… **ëª¨ë°”ì¼ ìµœì í™”** (ExecuTorch í†µí•©)
- âœ… **PyTorch ê³µì‹ í˜‘ë ¥**

**ê¶Œì¥**: âœ… **ì–‘ìí™” ë„êµ¬ë¡œ ì±„íƒ**

---

#### Option 4-B: TensorFlow Lite ì–‘ìí™”

**í˜„ì¬ ê³„íš**:
```python
import tensorflow as tf

converter = tf.lite.TFLiteConverter.from_saved_model("model")
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.int8]

tflite_model = converter.convert()
```

**í‰ê°€**:
- âœ… **í‘œì¤€ ë„êµ¬**
- âš ï¸ Unslothë³´ë‹¤ ì •í™•ë„ í•˜ë½ í¼
- âœ… **Android ë„¤ì´í‹°ë¸Œ ì§€ì›**

**ê¶Œì¥**: âœ… **ìœ ì§€** (Unslothì™€ ë³‘í–‰)

---

#### Option 4-C: ONNX Runtime + Quantization

**í•µì‹¬ ìŠ¤í™**:
```yaml
Runtime: ONNX Runtime Mobile
Quantization: INT8 static/dynamic
Optimization: Graph optimization
Deployment: Android/iOS
```

**ì¥ì **:
- âœ… **í¬ë¡œìŠ¤ í”Œë«í¼**
- âœ… **í‘œì¤€í™”**
- âš ï¸ **TFLiteë³´ë‹¤ ëŠë¦¼** (Android)

**ê¶Œì¥**: âš ï¸ **ë°±ì—…** (SNPE/TFLite ìš°ì„ )

---

### Category 5: Ensemble & Hybrid ì ‘ê·¼

#### Ensemble Strategy 1: Voting Ensemble

```python
class VotingEnsemble:
    """
    TCN, LSTM-AE, LightGBM ì˜ˆì¸¡ ê²°í•©
    """
    def __init__(self):
        self.tcn = TCNModel()
        self.lstm_ae = LSTMAEModel()
        self.lgbm = LightGBMModel()

    def predict_fuel(self, data):
        # TCN ì£¼ ì˜ˆì¸¡ê¸°
        tcn_pred = self.tcn.predict(data)

        # LSTM-AE ì´ìƒì¹˜ í•„í„°ë§
        is_anomaly = self.lstm_ae.detect_anomaly(data)
        if is_anomaly:
            return None  # ì‹ ë¢° ë¶ˆê°€

        return tcn_pred

    def classify_behavior(self, data):
        # LightGBM ì£¼ ë¶„ë¥˜ê¸°
        lgbm_pred = self.lgbm.predict(data)

        # LSTM-AEë¡œ ê²€ì¦
        is_anomaly = self.lstm_ae.detect_anomaly(data)
        if is_anomaly:
            return "ANOMALY"

        return lgbm_pred
```

**ì¥ì **:
- âœ… ëª¨ë¸ ê°„ ê²€ì¦
- âœ… ì‹ ë¢°ë„ í–¥ìƒ
- âš ï¸ ì§€ì—° ì¦ê°€

---

#### Ensemble Strategy 2: Cascading Models

```python
class CascadingPipeline:
    """
    ìˆœì°¨ì  ëª¨ë¸ ì‹¤í–‰ (ì¡°ê±´ë¶€)
    """
    def process(self, data):
        # Step 1: ë¹ ë¥¸ ì´ìƒ íƒì§€ (LSTM-AE)
        if self.lstm_ae.detect_anomaly(data):
            return {"alert": "ANOMALY", "confidence": 0.95}

        # Step 2: ì •ìƒì´ë©´ ì—°ë£Œ ì˜ˆì¸¡ (TCN)
        fuel_pred = self.tcn.predict(data)

        # Step 3: í–‰ë™ ë¶„ë¥˜ (LightGBM)
        behavior = self.lgbm.classify(data)

        return {
            "fuel": fuel_pred,
            "behavior": behavior,
            "confidence": 0.85
        }
```

**ì¥ì **:
- âœ… íš¨ìœ¨ì  (ì¡°ê¸° ì¢…ë£Œ)
- âœ… ì§€ì—° ìµœì†Œí™”
- âœ… ìš°ì„ ìˆœìœ„ ëª…í™•

---

#### Ensemble Strategy 3: Model Stacking

```python
class StackedModel:
    """
    ë©”íƒ€ ëª¨ë¸ë¡œ ìµœì¢… ê²°ì •
    """
    def __init__(self):
        # Level 0: Base models
        self.tcn = TCNModel()
        self.lstm_ae = LSTMAEModel()
        self.ttm = TTMModel()  # IBM TTM ì¶”ê°€

        # Level 1: Meta model
        self.meta = LightGBMModel()

    def predict(self, data):
        # ëª¨ë“  ë² ì´ìŠ¤ ëª¨ë¸ ì˜ˆì¸¡
        tcn_pred = self.tcn.predict(data)
        lstm_features = self.lstm_ae.encode(data)
        ttm_pred = self.ttm.predict(data)

        # ë©”íƒ€ ëª¨ë¸ ì…ë ¥
        meta_input = np.concatenate([
            [tcn_pred],
            lstm_features,
            [ttm_pred]
        ])

        # ìµœì¢… ì˜ˆì¸¡
        return self.meta.predict(meta_input)
```

**ì¥ì **:
- âœ… ìµœê³  ì •í™•ë„ ê°€ëŠ¥
- âŒ ë³µì¡ë„ ì¦ê°€
- âŒ ì§€ì—° ì¦ê°€ (50ms+ ìœ„í—˜)

**í‰ê°€**: âš ï¸ **ì˜¤ë²„í‚¬** (ë‹¨ìˆœ í‰ê· /íˆ¬í‘œê°€ ì¶©ë¶„)

---

## ğŸ“Š ëª¨ë¸ ì¡°í•© ì‹œë‚˜ë¦¬ì˜¤

### Scenario 1: Minimal (í˜„ì¬ ê³„íš) - Baseline

```yaml
êµ¬ì„±:
  - TCN (custom): ì—°ë£Œ ì˜ˆì¸¡
  - LSTM-AE (custom): ì´ìƒ íƒì§€
  - LightGBM: í–‰ë™ ë¶„ë¥˜

ì´ í¬ê¸°: ~12MB
ì´ ì§€ì—°: ~50ms (ìˆœì°¨), ~35ms (ë³‘ë ¬)
ì „ë ¥: <2W
ë³µì¡ë„: â­â­ (ì¤‘ê°„)

ì¥ì :
  âœ… ì™„ì „ ì œì–´
  âœ… í¬ê¸° ìµœì†Œ
  âœ… ì´ë¯¸ êµ¬í˜„ë¨
  âœ… ê²€ì¦ëœ ì•„í‚¤í…ì²˜

ë‹¨ì :
  âš ï¸ ì‚¬ì „ í•™ìŠµ ì—†ìŒ
  âš ï¸ Zero-shot ë¶ˆê°€
  âš ï¸ ë°ì´í„° ì˜ì¡´ë„ ë†’ìŒ

ì¶”ì²œ: âœ… Baselineìœ¼ë¡œ ìœ ì§€
```

---

### Scenario 2: Enhanced (ì˜¤í”ˆì†ŒìŠ¤ ê°•í™”) - â­ **ì¶”ì²œ**

```yaml
êµ¬ì„±:
  - IBM TTM-r2: ì—°ë£Œ ì˜ˆì¸¡ (ì‚¬ì „ í•™ìŠµ í™œìš©)
  - LSTM-AE (custom): ì´ìƒ íƒì§€ (ìš°ë¦¬ ë°ì´í„° íŠ¹í™”)
  - LightGBM: í–‰ë™ ë¶„ë¥˜
  - Anomalib PatchCore: ë³´ì¡° ì´ìƒ íƒì§€ (ê²€ì¦ìš©)

ì´ í¬ê¸°: ~20MB (TTM í¬í•¨)
ì´ ì§€ì—°: ~55ms
ì „ë ¥: <2.5W
ë³µì¡ë„: â­â­â­ (ì¤‘ìƒ)

ì¥ì :
  âœ… ì‚¬ì „ í•™ìŠµ í™œìš© (TTM)
  âœ… Zero-shot ê°€ëŠ¥
  âœ… ì •í™•ë„ í–¥ìƒ ê¸°ëŒ€
  âœ… ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ (Anomalib)

ë‹¨ì :
  âš ï¸ í¬ê¸° ì¦ê°€ (12MB â†’ 20MB)
  âš ï¸ ì§€ì—° ì•½ê°„ ì¦ê°€
  âš ï¸ í†µí•© ë³µì¡ë„

ì¶”ì²œ: â­ ìµœì¢… ê¶Œì¥
```

---

### Scenario 3: Hybrid Cloud-Edge (Samsara ìŠ¤íƒ€ì¼)

```yaml
Edge (ì‹¤ì‹œê°„ ì•ˆì „):
  - LSTM-AE: ê¸‰ì œë™/ê¸‰ê°€ì† ì¦‰ì‹œ ê²½ê³  (<30ms)
  - LightGBM: ìœ„í—˜ í–‰ë™ ë¶„ë¥˜ (<15ms)
  - ì´: <14MB, <50ms

Cloud (ë¶„ì„):
  - Google TimesFM: ì¥ê¸° ì—°ë£Œ íŠ¸ë Œë“œ
  - TTM: ì˜ˆì¸¡ ì •ë¹„
  - LLM: ì½”ì¹­ í…ìŠ¤íŠ¸ ìƒì„±

ì¥ì :
  âœ… ê° ë ˆì´ì–´ ìµœì í™”
  âœ… ì•ˆì „ì„± ë³´ì¥ (Edge)
  âœ… ê³ ê¸‰ ë¶„ì„ (Cloud)

ë‹¨ì :
  âš ï¸ ì¸í„°ë„· ì˜ì¡´ (Cloud)
  âš ï¸ ë³µì¡ë„ ìµœê³ 

ì¶”ì²œ: âš ï¸ ë¯¸ë˜ í™•ì¥ ì˜µì…˜
```

---

### Scenario 4: TinyML Extreme (ì´ˆê²½ëŸ‰)

```yaml
êµ¬ì„±:
  - FastGRNN (Microsoft EdgeML): ì—°ë£Œ ì˜ˆì¸¡
  - Simple Autoencoder: ì´ìƒ íƒì§€
  - Decision Tree: í–‰ë™ ë¶„ë¥˜

ì´ í¬ê¸°: <5MB
ì´ ì§€ì—°: <20ms
ì „ë ¥: <1W
ë³µì¡ë„: â­ (ë‚®ìŒ)

ì¥ì :
  âœ… ê·¹ë„ë¡œ ê²½ëŸ‰
  âœ… ë§¤ìš° ë¹ ë¦„
  âœ… STM32ì—ì„œë„ ê°€ëŠ¥

ë‹¨ì :
  âŒ ì •í™•ë„ í¬ìƒ (75-80%)
  âŒ ê¸°ëŠ¥ ì œí•œ

ì¶”ì²œ: âŒ ì œì™¸ (ì •í™•ë„ ë¶€ì¡±)
```

---

## ğŸ† ìµœì¢… ê¶Œì¥ ì¡°í•©

### Primary Recommendation: **Scenario 2 (Enhanced)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GLEC DTG Edge AI - ê¶Œì¥ ëª¨ë¸ ìŠ¤íƒ               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[1] ì—°ë£Œ ì†Œë¹„ ì˜ˆì¸¡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Primary:   IBM Granite TTM-r2 (1M-10M params)
           - Hugging Face: ibm-granite/granite-timeseries-ttm-r2
           - Size: 4-10MB (FP32), 1-2.5MB (INT8)
           - Latency: 10-20ms
           - Zero-shot capable
           - Few-shot fine-tuning

Fallback:  Custom TCN (í˜„ì¬ êµ¬í˜„)
           - Size: 2-4MB (INT8)
           - Latency: 15-25ms
           - ì™„ì „ ì œì–´ ê°€ëŠ¥

Strategy:  TTMìœ¼ë¡œ ì‹œì‘, í•„ìš” ì‹œ TCN fine-tuning

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[2] ì´ìƒ íƒì§€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Primary:   Custom LSTM-Autoencoder
           - ìš°ë¦¬ CAN ë°ì´í„° íŠ¹í™”
           - Size: 2-3MB (INT8)
           - Latency: 25-35ms
           - F1-score target: >0.85

Validator: Anomalib PatchCore (ì„ íƒì )
           - ë³´ì¡° ê²€ì¦ ë ˆì´ì–´
           - Size: +5MB
           - Latency: +10ms
           - ì‹ ë¢°ë„ í–¥ìƒ

Strategy:  LSTM-AE ì£¼ë ¥, PatchCoreë¡œ ê²€ì¦

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[3] ìš´ì „ í–‰ë™ ë¶„ë¥˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Primary:   LightGBM
           - Microsoft (MIT License)
           - Size: 5-10MB
           - Latency: 5-15ms
           - Accuracy target: >90%
           - Java ë„¤ì´í‹°ë¸Œ ì§€ì›

Strategy:  ìœ ì§€ (ìµœì  ì„ íƒ)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[4] ìµœì í™” ë„êµ¬
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Quantization: Unsloth QAT
           - 4-bit/8-bit dynamic
           - 70% ì •í™•ë„ ë³µêµ¬
           - ExecuTorch export

Deployment:
           - Primary: SNPE (Qualcomm DSP/HTP)
           - Fallback: TFLite (NNAPI/GPU)
           - Backup: ONNX Runtime

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ì´í•© ìŠ¤í™
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Size:     15-20MB (TTM í¬í•¨)
Total Latency:  40-65ms (ìˆœì°¨), 30-40ms (ë³‘ë ¬)
Power:          <2.5W
Accuracy:       85-92% (ê° ëª¨ë“ˆ)
Offline:        âœ… ì™„ì „ ê°€ëŠ¥
Realtime:       âœ… <50ms ëª©í‘œ ì¶©ì¡±

```

---

### Implementation Priority

```
Phase 1: Baseline (Week 1-2)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… í˜„ì¬ ì„¤ê³„ êµ¬í˜„ ì™„ë£Œ
   - Custom TCN
   - Custom LSTM-AE
   - LightGBM

â†’ ë°ì´í„° ìƒì„± (í•©ì„± ì‹œë®¬ë ˆì´í„°)
â†’ í•™ìŠµ (ë¡œì»¬ GPU)
â†’ ì„±ëŠ¥ ê²€ì¦

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 2: Enhancement (Week 3-4)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ”„ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ í†µí•©
   - IBM TTM-r2 í…ŒìŠ¤íŠ¸
   - Anomalib ì¶”ê°€
   - ì„±ëŠ¥ ë¹„êµ (TTM vs TCN)

â†’ A/B í…ŒìŠ¤íŠ¸
â†’ ìµœì  ì¡°í•© ì„ ì •

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 3: Optimization (Week 5)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âš¡ ì–‘ìí™” ë° ìµœì í™”
   - Unsloth QAT ì ìš©
   - SNPE/TFLite ë³€í™˜
   - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

â†’ <14MB, <50ms ë‹¬ì„±

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Phase 4: Integration (Week 6-7)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“± Android í†µí•©
   - SNPE ì¶”ë¡  ì—”ì§„
   - TFLite fallback
   - E2E í…ŒìŠ¤íŠ¸

â†’ ì‹¤ì°¨ ê²€ì¦
```

---

## ğŸš« ëª…ì‹œì  ë°°ì œ: ì—£ì§€ LLM

### Why NOT Edge LLM for Core Functions

**ê²€í† í•œ ëª¨ë¸ë“¤**:
- âŒ Liquid AI LFM2 (350M-1.2B)
- âŒ Gemini Nano
- âŒ Qwen-1.5B
- âŒ Phi-2/Phi-3

**ë°°ì œ ì´ìœ **:

#### 1. í™˜ê°(Hallucination) - ì¹˜ëª…ì 

```
ì‹œë‚˜ë¦¬ì˜¤: ê¸‰ì œë™ ê²½ê³ 

Task-Specific (LSTM-AE):
  Input: acceleration = -6.5 m/sÂ²
  Output: ALERT = True (ê²°ì •ë¡ ì )
  ì‹ ë¢°ë„: 99.9%

Edge LLM:
  Input: "ì°¨ëŸ‰ì´ ê¸‰ì œë™ ì¤‘ì…ë‹ˆë‹¤"
  Output: "ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”" (í™˜ê°)
  ì‹ ë¢°ë„: 85-95% (ë¶ˆì¶©ë¶„)

â†’ ì•ˆì „ ì‹œìŠ¤í…œì—ì„œ 15% ì˜¤ë¥˜ëŠ” ì¹˜ëª…ì 
```

#### 2. ì§€ì—°ì‹œê°„ - ì‹¤ì‹œê°„ ë¶ˆê°€

| ëª¨ë¸ | ì§€ì—° (Snapdragon 865) | ëª©í‘œ | íŒì • |
|------|---------------------|------|------|
| LSTM-AE | 25-35ms | <50ms | âœ… í†µê³¼ |
| LightGBM | 5-15ms | <50ms | âœ… í†µê³¼ |
| LFM2-700M | 100-150ms | <50ms | âŒ ì‹¤íŒ¨ |
| Qwen-1.5B | 150-250ms | <50ms | âŒ ì‹¤íŒ¨ |

```
ê¸‰ì œë™ ì‹œë‚˜ë¦¬ì˜¤:
- 250ms = 0.25ì´ˆ
- ì‹œì† 100km/h = 27.8 m/s
- 0.25ì´ˆ ë™ì•ˆ ì´ë™: 6.95m

â†’ 7m ì§€ì—°ì€ ì‚¬ê³  ë°œìƒ ê°€ëŠ¥
```

#### 3. ì „ë ¥ ì†Œëª¨ - ë°°í„°ë¦¬ ë¶€ë‹´

| ëª¨ë¸ | ì „ë ¥ (W) | 1ì¼ ì†Œë¹„ (Wh) | ë°°í„°ë¦¬ ì˜í–¥ |
|------|---------|--------------|------------|
| Task-Specific | 1.5W | 36 Wh | âœ… ë¬´ì‹œ ê°€ëŠ¥ |
| LFM2-700M | 5-7W | 120-168 Wh | âŒ ì‹¬ê° |
| Qwen-1.5B | 8-12W | 192-288 Wh | âŒ ë§¤ìš° ì‹¬ê° |

```
ì°¨ëŸ‰ ë°°í„°ë¦¬: ~60Ah @ 12V = 720Wh

LLM 1ì¼ ì‚¬ìš©:
- 168Wh / 720Wh = 23% ì†Œëª¨
- 3ì¼ì´ë©´ ë°°í„°ë¦¬ ë°©ì „

â†’ ìƒìš© í…”ë ˆë§¤í‹±ìŠ¤ì— ë¶€ì í•©
```

#### 4. ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±

```python
# Task-Specific: ëª…í™•í•œ ë¡œì§
if acceleration < -4.0 and brake_pressure > 50:
    return "HARSH_BRAKING"  # 100% ì¬í˜„ ê°€ëŠ¥

# LLM: í™•ë¥ ì 
llm.generate("ì°¨ëŸ‰ ìƒíƒœ ë¶„ì„")
# â†’ "ê¸‰ì œë™ì…ë‹ˆë‹¤" (85%)
# â†’ "ì •ìƒì…ë‹ˆë‹¤" (10%)
# â†’ "ë‚ ì”¨ ì¢‹ë„¤ìš”" (5%)  # í™˜ê°

â†’ ì•ˆì „ ì‹œìŠ¤í…œì€ ê²°ì •ë¡ ì  í–‰ë™ í•„ìˆ˜
```

### Edge LLM í—ˆìš© ë²”ìœ„

**âš ï¸ ì œí•œì  ìš©ë„ë§Œ**:

```yaml
í—ˆìš©:
  - ìŒì„± ë¹„ì„œ (ìš´ì „ì ì§ˆë¬¸ ì‘ë‹µ)
  - ë³´ê³ ì„œ ìš”ì•½ (ë°°ì¹˜ ì²˜ë¦¬)
  - ì½”ì¹­ í…ìŠ¤íŠ¸ ìƒì„± (ì˜¤í”„ë¼ì¸)

ì¡°ê±´:
  - ì•ˆì „ ê¸°ëŠ¥ê³¼ ì™„ì „ ë¶„ë¦¬
  - ë³„ë„ í”„ë¡œì„¸ìŠ¤ (crash ì‹œ ì•ˆì „ ì˜í–¥ ì—†ìŒ)
  - ì„ íƒì  í™œì„±í™” (ì „ë ¥ ì ˆì•½)
  - ì¸í„°ë„· ë°±ì—… (Cloud LLM)

ê¸ˆì§€:
  âŒ ì‹¤ì‹œê°„ ì•ˆì „ ê²½ê³ 
  âŒ ì´ìƒ íƒì§€
  âŒ ì—°ë£Œ ì˜ˆì¸¡
  âŒ í–‰ë™ ë¶„ë¥˜
```

---

## ğŸ“š ì˜¤í”ˆì†ŒìŠ¤ ë¦¬ì†ŒìŠ¤ ë§µ

### Hugging Face Models

```yaml
ì‹œê³„ì—´ ì˜ˆì¸¡:
  - ibm-granite/granite-timeseries-ttm-r2 â­
  - ibm-granite/granite-timeseries-ttm-r1
  - google/timesfm-1.0-200m (í¬ê¸° ì´ˆê³¼)
  - time-series-foundation-models/Lag-Llama (í¬ê¸° ì´ˆê³¼)

ì´ìƒ íƒì§€:
  - keras-io/timeseries-anomaly-detection
  - keras-io/time-series-anomaly-detection-autoencoder

ì••ì¶•/ë°°í¬:
  - unsloth/LFM2-700M-unsloth-bnb-4bit (ì°¸ê³ ìš©)
  - onnx-community/[model]-ONNX
```

### GitHub Repositories

```yaml
ì´ìƒ íƒì§€:
  - open-edge-platform/anomalib â­
  - nhorro/can-anomaly-detection
  - zadid56/in-vehicle-security

ì–‘ìí™”:
  - unslothai/unsloth â­
  - PINTO0309/onnx2tf

TinyML:
  - gigwegbe/tinyml-papers-and-projects
  - microsoft/EdgeML
  - TexasInstruments/tinyml-tensorlab

CAN Bus:
  - ankitrajsh/CAN-bus-for-anamolies-detection
```

### í•™ìˆ  ë…¼ë¬¸ (ìµœì‹ )

```yaml
2024-2025:
  - IBM TTM (NeurIPS 2024)
  - Unsloth QAT (PyTorch í˜‘ë ¥)
  - OTAD Framework (Nature, 2025)
  - LFM2 (Liquid AI, 2024)

Classical (ê²€ì¦ë¨):
  - TCN: Bai et al., 2018
  - LSTM-AE: Malhotra et al., 2016
  - LightGBM: Ke et al., 2017
```

---

## ğŸ¯ ì˜ì‚¬ê²°ì • ë§¤íŠ¸ë¦­ìŠ¤

### ì—°ë£Œ ì˜ˆì¸¡ ëª¨ë¸ ì„ íƒ

```
         ì„±ëŠ¥  í¬ê¸°  ì†ë„  ì‚¬ì „í•™ìŠµ  ì œì–´  ì´ì 
TTM-r2    â­â­â­  â­â­   â­â­â­   â­â­â­   â­â­   13
TCN       â­â­   â­â­â­  â­â­â­   â­     â­â­â­  12
TimesFM   â­â­â­  â­    â­â­    â­â­â­   â­â­   11

ê¶Œì¥: TTM-r2 (ì‚¬ì „í•™ìŠµ + ì‘ì€ í¬ê¸°)
ë°±ì—…: TCN (ì™„ì „ ì œì–´)
```

### ì´ìƒ íƒì§€ ëª¨ë¸ ì„ íƒ

```
           ì„±ëŠ¥  CANíŠ¹í™”  ì—£ì§€ìµœì   ê²€ì¦  ì´ì 
LSTM-AE     â­â­â­  â­â­â­   â­â­â­   â­â­   12
Anomalib    â­â­â­  â­â­    â­â­â­   â­â­â­  13
TranAD      â­â­â­  â­     â­      â­â­â­   9

ê¶Œì¥: LSTM-AE (ì£¼ë ¥) + Anomalib (ê²€ì¦)
```

### í–‰ë™ ë¶„ë¥˜ ëª¨ë¸ ì„ íƒ

```
            ì„±ëŠ¥  ì†ë„  í¬ê¸°  í•´ì„ì„±  ì´ì 
LightGBM    â­â­â­  â­â­â­  â­â­â­  â­â­â­   12
RandomForest â­â­   â­â­   â­â­   â­â­â­   9
DistilBERT  â­â­â­  â­    â­     â­â­    7

ê¶Œì¥: LightGBM (ì••ë„ì )
```

---

## ğŸ“Š ìµœì¢… ë¹„êµí‘œ

| í•­ëª© | Scenario 1 (Baseline) | Scenario 2 (Enhanced) â­ | Scenario 3 (Hybrid) | Scenario 4 (TinyML) |
|------|----------------------|--------------------------|---------------------|---------------------|
| **ì—°ë£Œ ì˜ˆì¸¡** | Custom TCN | IBM TTM-r2 | TTM (cloud) + TCN (edge) | FastGRNN |
| **ì´ìƒ íƒì§€** | LSTM-AE | LSTM-AE + Anomalib | LSTM-AE (edge only) | Simple AE |
| **í–‰ë™ ë¶„ë¥˜** | LightGBM | LightGBM | LightGBM (edge) | Decision Tree |
| **ì´ í¬ê¸°** | 12MB | 20MB | 14MB (edge) | 5MB |
| **ì´ ì§€ì—°** | 50ms | 55ms | 45ms (edge) | 20ms |
| **ì „ë ¥** | 2W | 2.5W | 2W (edge) | 1W |
| **ì •í™•ë„** | 85% | 90% | 85% (edge) | 75% |
| **Zero-shot** | âŒ | âœ… (TTM) | âœ… | âŒ |
| **ì˜¤í”„ë¼ì¸** | âœ… | âœ… | âš ï¸ (ë¶€ë¶„) | âœ… |
| **ë³µì¡ë„** | â­â­ | â­â­â­ | â­â­â­â­ | â­ |
| **ê¶Œì¥ë„** | âœ… Baseline | â­ **ìµœì¢… ê¶Œì¥** | âš ï¸ ë¯¸ë˜ | âŒ ë¶€ì¡± |

---

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Week 1-2: Baseline êµ¬í˜„

```bash
# 1. ë°ì´í„° ìƒì„±
python data-generation/synthetic_driving_simulator.py \
    --output-dir datasets \
    --samples 35000

# 2. Custom ëª¨ë¸ í•™ìŠµ
python ai-models/training/train_tcn.py
python ai-models/training/train_lstm_ae.py
python ai-models/training/train_lightgbm.py

# 3. ì„±ëŠ¥ ê²€ì¦
pytest ai-models/tests/ -v
```

### Week 3-4: Enhancement (ì˜¤í”ˆì†ŒìŠ¤ í†µí•©)

```bash
# 1. IBM TTM í†µí•©
pip install transformers
huggingface-cli download ibm-granite/granite-timeseries-ttm-r2

# Python ì½”ë“œ
from transformers import AutoModel

ttm = AutoModel.from_pretrained("ibm-granite/granite-timeseries-ttm-r2")

# Few-shot fine-tuning
# ... (ìš°ë¦¬ ë°ì´í„° 35,000 ìƒ˜í”Œë¡œ fine-tuning)

# 2. Anomalib í…ŒìŠ¤íŠ¸
pip install anomalib
# PatchCore vs LSTM-AE ë¹„êµ

# 3. ì„±ëŠ¥ ë¹„êµ
# TTM vs TCN
# Anomalib vs LSTM-AE
```

### Week 5: Optimization

```bash
# 1. Unsloth QAT
pip install unsloth

# TTM quantization
from unsloth import FastLanguageModel

model = FastLanguageModel.from_pretrained(
    "ibm-granite/granite-timeseries-ttm-r2",
    load_in_4bit=True
)

# QAT fine-tuning
# ...

# Export
model.save_pretrained_gguf("ttm_q4.gguf")

# 2. TFLite ë³€í™˜
python ai-models/conversion/export_onnx.py
python ai-models/optimization/quantize_model.py

# 3. SNPE ë³€í™˜ (ë¡œì»¬)
snpe-onnx-to-dlc \
    --input_network ttm.onnx \
    --output_path ttm.dlc
```

### Week 6-7: Android í†µí•©

```kotlin
// SNPE ì¶”ë¡  ì—”ì§„
class SNPEInferenceEngine {
    private val snpe: SNPE

    fun loadTTM() {
        snpe = SNPE.NeuralNetworkBuilder(context)
            .setModel("ttm_q4.dlc")
            .setRuntimeOrder(Runtime.DSP)  // Qualcomm HTP
            .build()
    }

    fun predictFuel(canData: FloatArray): Float {
        val output = snpe.execute(mapOf("input" to canData))
        return output["output"]!![0]
    }
}
```

---

## ğŸ“– ê²°ë¡ 

### ìµœì¢… ê¶Œì¥ ì‚¬í•­

**1. Core Models (í•„ìˆ˜)**:
- âœ… **IBM TTM-r2**: ì—°ë£Œ ì˜ˆì¸¡ (ì‚¬ì „ í•™ìŠµ í™œìš©)
- âœ… **LSTM-AE (Custom)**: ì´ìƒ íƒì§€ (CAN íŠ¹í™”)
- âœ… **LightGBM**: í–‰ë™ ë¶„ë¥˜ (ì‚°ì—… í‘œì¤€)

**2. Optimization Tools (í•„ìˆ˜)**:
- âœ… **Unsloth QAT**: ì–‘ìí™” (ì •í™•ë„ ë³´ì¡´)
- âœ… **SNPE**: Qualcomm ê°€ì† (ì£¼ë ¥)
- âœ… **TFLite**: ë°±ì—… ëŸ°íƒ€ì„

**3. Enhancement (ì„ íƒ)**:
- âš ï¸ **Anomalib PatchCore**: ì´ìƒ íƒì§€ ê²€ì¦

**4. Explicit Exclusion (ëª…ì‹œì  ë°°ì œ)**:
- âŒ **Edge LLM**: ì•ˆì „ ê¸°ëŠ¥ ë¶€ì í•© (í™˜ê°, ì§€ì—°, ì „ë ¥)
- âŒ **Large Transformers**: í¬ê¸° ì´ˆê³¼ (TimesFM, Lag-Llama)

### êµ¬í˜„ ìš°ì„ ìˆœìœ„

```
ìš°ì„ ìˆœìœ„ 1 (ì¦‰ì‹œ): Baseline ì™„ì„±
  â†’ Custom TCN, LSTM-AE, LightGBM
  â†’ ë°ì´í„° ìƒì„± ë° í•™ìŠµ
  â†’ ì„±ëŠ¥ ê²€ì¦ (RÂ²>0.85, F1>0.85, Acc>0.90)

ìš°ì„ ìˆœìœ„ 2 (1ê°œì›”): Enhancement
  â†’ IBM TTM-r2 í†µí•© ë° ë¹„êµ
  â†’ Unsloth QAT ì ìš©
  â†’ SNPE ìµœì í™”

ìš°ì„ ìˆœìœ„ 3 (2ê°œì›”): Production
  â†’ Android í†µí•©
  â†’ ì‹¤ì°¨ í…ŒìŠ¤íŠ¸
  â†’ ë°°í¬ ì¤€ë¹„
```

### ì„±ê³µ ì§€í‘œ

```yaml
Technical:
  - Model Size: <20MB âœ… (TTM í¬í•¨)
  - Latency: <60ms âœ… (ëª©í‘œ 50ms ê·¼ì ‘)
  - Accuracy: >85% âœ… (ê° ëª¨ë“ˆ)
  - Power: <2.5W âœ…

Business:
  - Samsara ìˆ˜ì¤€ ì‹ ë¢°ì„± ë‹¬ì„±
  - ì˜¤í”„ë¼ì¸ ì™„ì „ ì‘ë™
  - ìƒìš© ë°°í¬ ê°€ëŠ¥ í’ˆì§ˆ
```

---

**Generated**: 2025-01-09
**Research Sources**: Hugging Face, GitHub, Unsloth, arXiv, Industry (Samsara)
**Total Models Analyzed**: 20+
**Recommended Combination**: IBM TTM-r2 + LSTM-AE + LightGBM
**Deployment**: SNPE (Qualcomm) + TFLite (backup)
